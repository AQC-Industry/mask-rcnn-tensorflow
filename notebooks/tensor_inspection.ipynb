{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tensor Inspection\n",
    "\n",
    "This notebook details the process of identifying and tracking the values of tensors in a given network with an example using Mask RCNN. First, we generate a small dataset consisting of a single image from the coco data. We then look at how to track the tensors within Mask RCNN using that image. Finally, we track the gradients that backpropogate through the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/callbacks/hooks.py:17: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/tfutils/optimizer.py:19: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/tfutils/sesscreate.py:24: The name tf.train.SessionCreator is deprecated. Please use tf.compat.v1.train.SessionCreator instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod-0.18.0-py3.6-linux-x86_64.egg/horovod/tensorflow/__init__.py:117: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod-0.18.0-py3.6-linux-x86_64.egg/horovod/tensorflow/__init__.py:143: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/lib/python3.6/dist-packages/numba/errors.py:131: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "from ast import literal_eval\n",
    "import json\n",
    "#os.environ['TF_CUDNN_DETERMINISTIC'] = 'true'\n",
    "os.environ['TENSORPACK_FP16'] = 'true'\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    import tensorflow as tf\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import tensorpack.utils.viz as tpviz\n",
    "from tensorpack import *\n",
    "from tensorpack.tfutils.common import get_tf_version_tuple\n",
    "sys.path.append('/mask-rcnn-tensorflow/MaskRCNN')\n",
    "from model.generalized_rcnn import ResNetFPNModel\n",
    "from config import finalize_configs, config as cfg\n",
    "from eval import DetectionResult, predict_image, multithread_predict_dataflow, EvalCallback\n",
    "from performance import ThroughputTracker, humanize_float\n",
    "from data import get_eval_dataflow, get_train_dataflow, get_batch_train_dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = ResNetFPNModel(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.DATA.BASEDIR = '/data/small_sample/'\n",
    "cfg.BACKBONE.WEIGHTS = '/data/pretrained-models/ImageNet-R50-AlignPadding.npz'\n",
    "tf.set_random_seed(cfg.TRAIN.SEED)\n",
    "fix_rng_seed(cfg.TRAIN.SEED)\n",
    "np.random.seed(cfg.TRAIN.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train dataflow\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[1114 11:24:41 @dataset.py:50]\u001b[0m Instances loaded from /data/small_sample/annotations/instances_train2017.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 7270.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1114 11:24:41 @timer.py:50]\u001b[0m Load Load annotations for train2017 finished, time:0.0053sec.\n",
      "Done loading roidbs\n",
      "\u001b[32m[1114 11:24:41 @data.py:618]\u001b[0m Filtered 0 images which contain no non-crowd groudtruth boxes. Total #images for training: 25\n",
      "Batching roidbs\n",
      "Done batching roidbs\n",
      "\u001b[32m[1114 11:24:41 @config.py:285]\u001b[0m Config: ------------------------------------------\n",
      "{'BACKBONE': {'FREEZE_AFFINE': False,\n",
      "              'FREEZE_AT': 2,\n",
      "              'NORM': 'FreezeBN',\n",
      "              'RESNET_NUM_BLOCKS': [3, 4, 6, 3],\n",
      "              'STRIDE_1X1': False,\n",
      "              'TF_PAD_MODE': False,\n",
      "              'WEIGHTS': '/data/pretrained-models/ImageNet-R50-AlignPadding.npz'},\n",
      " 'DATA': {'BASEDIR': '/data/small_sample/',\n",
      "          'CLASS_NAMES': ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
      "                          'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n",
      "                          'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
      "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag',\n",
      "                          'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n",
      "                          'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
      "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon',\n",
      "                          'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot',\n",
      "                          'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant',\n",
      "                          'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
      "                          'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
      "                          'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
      "                          'hair drier', 'toothbrush'],\n",
      "          'NUM_CATEGORY': 80,\n",
      "          'NUM_CLASS': 81,\n",
      "          'TRAIN': ['train2017'],\n",
      "          'VAL': ('val2017',)},\n",
      " 'FPN': {'ANCHOR_STRIDES': (4, 8, 16, 32, 64),\n",
      "         'BOXCLASS_CONV_HEAD_DIM': 256,\n",
      "         'BOXCLASS_FC_HEAD_DIM': 1024,\n",
      "         'BOXCLASS_HEAD_FUNC': 'boxclass_2fc_head',\n",
      "         'MRCNN_HEAD_FUNC': 'maskrcnn_up4conv_head',\n",
      "         'NORM': 'None',\n",
      "         'NUM_CHANNEL': 256,\n",
      "         'PROPOSAL_MODE': 'Level',\n",
      "         'RESOLUTION_REQUIREMENT': 32},\n",
      " 'FRCNN': {'BATCH_PER_IM': 512,\n",
      "           'BBOX_REG_WEIGHTS': [10.0, 10.0, 5.0, 5.0],\n",
      "           'FG_RATIO': 0.25,\n",
      "           'FG_THRESH': 0.5},\n",
      " 'MODE_FPN': True,\n",
      " 'MODE_MASK': True,\n",
      " 'MRCNN': {'HEAD_DIM': 256},\n",
      " 'PREPROC': {'MAX_SIZE': 1344.0,\n",
      "             'PADDING_SHAPES': [(800, 1000), (800, 1200), (800, 1350)],\n",
      "             'PIXEL_MEAN': [123.675, 116.28, 103.53],\n",
      "             'PIXEL_STD': [58.395, 57.12, 57.375],\n",
      "             'PREDEFINED_PADDING': False,\n",
      "             'TEST_SHORT_EDGE_SIZE': 800,\n",
      "             'TRAIN_SHORT_EDGE_SIZE': [800, 800]},\n",
      " 'RPN': {'ANCHOR_RATIOS': (0.5, 1.0, 2.0),\n",
      "         'ANCHOR_SIZES': (32, 64, 128, 256, 512),\n",
      "         'ANCHOR_STRIDE': 16,\n",
      "         'BATCH_PER_IM': 256,\n",
      "         'CROWD_OVERLAP_THRESH': 9.99,\n",
      "         'FG_RATIO': 0.5,\n",
      "         'HEAD_DIM': 1024,\n",
      "         'MIN_SIZE': 0.1,\n",
      "         'NEGATIVE_ANCHOR_THRESH': 0.3,\n",
      "         'NUM_ANCHOR': 15,\n",
      "         'POSITIVE_ANCHOR_THRESH': 0.7,\n",
      "         'PROPOSAL_NMS_THRESH': 0.7,\n",
      "         'SLOW_ACCURATE_MASK': True,\n",
      "         'TEST_PER_LEVEL_NMS_TOPK': 1000,\n",
      "         'TEST_POST_NMS_TOPK': 1000,\n",
      "         'TEST_PRE_NMS_TOPK': 6000,\n",
      "         'TOPK_PER_IMAGE': True,\n",
      "         'TRAIN_PER_LEVEL_NMS_TOPK': 2000,\n",
      "         'TRAIN_POST_NMS_TOPK': 2000,\n",
      "         'TRAIN_PRE_NMS_TOPK': 12000,\n",
      "         'UNQUANTIZED_ANCHOR': True},\n",
      " 'TEST': {'BOX_TARGET': 0.377,\n",
      "          'FRCNN_NMS_THRESH': 0.5,\n",
      "          'MASK_TARGET': 0.339,\n",
      "          'RESULTS_PER_IM': 100,\n",
      "          'RESULT_SCORE_THRESH': 0.05,\n",
      "          'RESULT_SCORE_THRESH_VIS': 0.3},\n",
      " 'TRAIN': {'BACKBONE_NCHW': True,\n",
      "           'BASE_LR': 0.00125,\n",
      "           'BATCH_SIZE_PER_GPU': 1,\n",
      "           'EVAL_PERIOD': 25,\n",
      "           'FPN_NCHW': True,\n",
      "           'GRADIENT_CLIP': 0,\n",
      "           'LR_EPOCH_SCHEDULE': [(8, 0.1), (10, 0.01), (12, None)],\n",
      "           'MASK_NCHW': True,\n",
      "           'NUM_GPUS': 8,\n",
      "           'RPN_NCHW': True,\n",
      "           'SEED': 1234,\n",
      "           'SHOULD_STOP': False,\n",
      "           'STARTING_EPOCH': 1,\n",
      "           'WARMUP_INIT_LR': 0.00041250000000000005,\n",
      "           'WARMUP_STEPS': 1000,\n",
      "           'WEIGHT_DECAY': 0.0001},\n",
      " 'TRAINER': 'replicated'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataflow = get_batch_train_dataflow(cfg.TRAIN.BATCH_SIZE_PER_GPU)\n",
    "finalize_configs(is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/tfutils/sessinit.py:259: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session_init = get_model_loader(cfg.BACKBONE.WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have our data and the model is setup. In order to export a gradient from the graph, we need to add a print statement to the tensor we want to track. For example, say we want to export the second to last layer of the backbone network (c4). Add this import to the top of the backbone.py file:\n",
    "\n",
    "```from performance import print_runtime_tensor, print_runtime_tensor_loose_branch```\n",
    "\n",
    "The, just after the c4 tensor is created in the network, add this line:\n",
    "\n",
    "```c4 = print_runtime_tensor(\\\"tensor_c4_forward\\\", c4)```\n",
    "\n",
    "Similarly, say we want to output a list of tensors. Perhaps the full output of the fpn (p23456). We can use something like:\n",
    "\n",
    "```p23456 = [print_runtime_tensor(\\\"tensor_p23456_{}_forward\\\".format(i), j) for i,j in enumerate(p23456)]```\n",
    "\n",
    "On the other hand, we might want to see a tensor that isn't actually used later in the graph, which means it wouldn't normally execute such that we can output it. This can be dome using the\n",
    "\n",
    "```print_runtime_tensor_loose_branch```\n",
    "\n",
    "For this, you need a downstream trigger tesnor to force the print of the tensor of interest. Say we have a tensor `t5` that isn't used in the graph, but `t1` is. We can print `t5` with:\n",
    "\n",
    "```t1 = print_runtime_tensor_loose_branch(\\\"tensor_t5_forward\\\", t5, trigger_tensor=t1)```\n",
    "\n",
    "Finally, say we want to print the gradients of the backwards pass. This is a little more complicated. Add this gradient printer class to the generalized_rcnn.py file:\n",
    "\n",
    "```\n",
    "class GradientPrinter(tf.train.Optimizer):\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "    def compute_gradients(self, *args, **kwargs):\n",
    "        return self.opt.compute_gradients(*args, **kwargs)\n",
    "    def apply_gradients(self, gradvars, global_step=None, name=None):\n",
    "        old_grads, v = zip(*gradvars)\n",
    "        old_grads = [print_runtime_tensor(\"tensor_{}_backward\".format(i.name), j) for i,j in zip(v, old_grads)]\n",
    "        gradvars = list(zip(old_grads, v))\n",
    "        return self.opt.apply_gradients(gradvars, global_step, name)\n",
    "```\n",
    "\n",
    "Inside the detection model class, modify the optimizer to pass through the gradient printer.\n",
    "\n",
    "```\n",
    "opt = tf.train.MomentumOptimizer(lr, 0.9)\n",
    "opt = GradientPrinter(opt)\n",
    "```\n",
    "\n",
    "Once the print function has been added, run the paragraph below with the capture magic function to catch the printed output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed example\n",
    "\n",
    "Let's work through all the steps of tracking a few tensors in the graph, using the fpn example above. First, open the `generalized_rcnn.py` file in the mask rcnn model. \n",
    "\n",
    "Add the import for to print a runtime tensor.\n",
    "\n",
    "<img src=\"assets/import_printer.png\" style=\"width: 600px;\">\n",
    "\n",
    "At the end of the fpn, add the print_runtime_tensor function. Remember that this function returns the same tensor (which is necessary to put the print command in the graph). Also, the p23456 object output from the fpn is a list, so we can print all tensors in the list with\n",
    "\n",
    "<img src=\"assets/print_comprehension.png\" style=\"width: 800px;\">\n",
    "\n",
    "If you want to get more information on this tensor, you can also find it in the graph using tensorboard. Based on the fpn file, these tensors follow the naming convention `posthoc_3x3_p{}` for p2-5 while p6 is named with `max_pool`. To use tensorboard, navigate to the log directory of you model, then select tensorboard with current directory from the new menu in the upper right corner.\n",
    "\n",
    "<img src=\"assets/start_tensorboard.png\" style=\"width: 800px;\">\n",
    "\n",
    "This will open tensorboard in a new tab. It might take a few seconds to load, since this is a large model. Once it's loaded you can search `posthoc_3x3` or `max_pool` under the graphs tab to find the fpn. The entire fpn subgraph looks like\n",
    "\n",
    "<img src=\"assets/tensorboard.png\" style=\"width: 800px;\">\n",
    "\n",
    "Before we run the model and get these tensors, let's also add in a print function for the backwards pass, so we can get the gradients. This is a little trickier, because it requires including a new optimizer, but you can follow the example from above. In the `generalized_rcnn.py` file, add this to the section just below the GradientClipOptimizer\n",
    "\n",
    "<img src=\"assets/add_optimizer.png\" style=\"width: 800px;\">\n",
    "\n",
    "Then include the optimizer just after the initial model optimizer.\n",
    "\n",
    "<img src=\"assets/call_optimizer.png\" style=\"width: 800px;\">\n",
    "\n",
    "Now when you run the model, the gradients in p23456 and all backward pass gradients will print to stdout. This is a lot of data that we would rather not have printing as the model runs, so within this notebook, we can supress and capture the output by adding `%%capture cap_out --no-stderr` at the top of the cell running the model. Also, it's a good idea to initially only run a single forward and backward pass by setting `steps_per_epoch` and `max_epoch` to 1.\n",
    "\n",
    "<img src=\"assets/model_config.png\" style=\"width: 600px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/tfutils/common.py:36: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "traincfg = TrainConfig(\n",
    "            model=MODEL,\n",
    "            data=QueueInput(train_dataflow),\n",
    "            steps_per_epoch=1,\n",
    "            max_epoch=1,\n",
    "            session_init=session_init,\n",
    "            session_config=None,\n",
    "            starting_epoch=cfg.TRAIN.STARTING_EPOCH\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SimpleTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/MaskRCNN/model/generalized_rcnn.py:148: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/input_source/input_source.py:219: The name tf.FIFOQueue is deprecated. Please use tf.queue.FIFOQueue instead.\n",
      "\n",
      "\u001b[32m[1114 11:24:44 @input_source.py:222]\u001b[0m Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/tfutils/summary.py:266: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "\u001b[32m[1114 11:24:44 @trainers.py:49]\u001b[0m Building graph for a single training tower ...\n",
      "\u001b[32m[1114 11:24:44 @registry.py:127]\u001b[0m conv0 input: [None, 3, None, None]\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/models/conv2d.py:90: The name tf.layers.Conv2D is deprecated. Please use tf.compat.v1.layers.Conv2D instead.\n",
      "\n",
      "\u001b[32m[1114 11:24:44 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/models/batch_norm.py:196: The name tf.layers.BatchNormalization is deprecated. Please use tf.compat.v1.layers.BatchNormalization instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\u001b[32m[1114 11:24:45 @registry.py:135]\u001b[0m conv0 output: [None, 64, None, None]\n",
      "\u001b[32m[1114 11:24:45 @registry.py:127]\u001b[0m pool0 input: [None, 64, None, None]\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/models/pool.py:35: The name tf.layers.MaxPooling2D is deprecated. Please use tf.compat.v1.layers.MaxPooling2D instead.\n",
      "\n",
      "\u001b[32m[1114 11:24:45 @registry.py:135]\u001b[0m pool0 output: [None, 64, None, None]\n",
      "\u001b[32m[1114 11:24:45 @registry.py:127]\u001b[0m group0/block0/conv1 input: [None, 64, None, None]\n",
      "\u001b[32m[1114 11:24:45 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:45 @registry.py:135]\u001b[0m group0/block0/conv1 output: [None, 64, None, None]\n",
      "\u001b[32m[1114 11:24:45 @registry.py:127]\u001b[0m group0/block0/conv2 input: [None, 64, None, None]\n",
      "\u001b[32m[1114 11:24:45 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:45 @registry.py:135]\u001b[0m group0/block0/conv2 output: [None, 64, None, None]\n",
      "\u001b[32m[1114 11:24:45 @registry.py:127]\u001b[0m group0/block0/conv3 input: [None, 64, None, None]\n",
      "\u001b[32m[1114 11:24:45 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:45 @registry.py:135]\u001b[0m group0/block0/conv3 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:45 @registry.py:127]\u001b[0m group0/block0/convshortcut input: [None, 64, None, None]\n",
      "\u001b[32m[1114 11:24:45 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:45 @registry.py:135]\u001b[0m group0/block0/convshortcut output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:45 @registry.py:127]\u001b[0m group0/block1/conv1 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:45 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:45 @registry.py:135]\u001b[0m group0/block1/conv1 output: [None, 64, None, None]\n",
      "\u001b[32m[1114 11:24:45 @registry.py:127]\u001b[0m group0/block1/conv2 input: [None, 64, None, None]\n",
      "\u001b[32m[1114 11:24:45 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:45 @registry.py:135]\u001b[0m group0/block1/conv2 output: [None, 64, None, None]\n",
      "\u001b[32m[1114 11:24:45 @registry.py:127]\u001b[0m group0/block1/conv3 input: [None, 64, None, None]\n",
      "\u001b[32m[1114 11:24:45 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:45 @registry.py:135]\u001b[0m group0/block1/conv3 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:45 @registry.py:127]\u001b[0m group0/block2/conv1 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:45 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:45 @registry.py:135]\u001b[0m group0/block2/conv1 output: [None, 64, None, None]\n",
      "\u001b[32m[1114 11:24:45 @registry.py:127]\u001b[0m group0/block2/conv2 input: [None, 64, None, None]\n",
      "\u001b[32m[1114 11:24:45 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:45 @registry.py:135]\u001b[0m group0/block2/conv2 output: [None, 64, None, None]\n",
      "\u001b[32m[1114 11:24:45 @registry.py:127]\u001b[0m group0/block2/conv3 input: [None, 64, None, None]\n",
      "\u001b[32m[1114 11:24:45 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:45 @registry.py:135]\u001b[0m group0/block2/conv3 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:45 @registry.py:127]\u001b[0m group1/block0/conv1 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:45 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:45 @registry.py:135]\u001b[0m group1/block0/conv1 output: [None, 128, None, None]\n",
      "\u001b[32m[1114 11:24:45 @registry.py:127]\u001b[0m group1/block0/conv2 input: [None, 128, None, None]\n",
      "\u001b[32m[1114 11:24:45 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:45 @registry.py:135]\u001b[0m group1/block0/conv2 output: [None, 128, None, None]\n",
      "\u001b[32m[1114 11:24:45 @registry.py:127]\u001b[0m group1/block0/conv3 input: [None, 128, None, None]\n",
      "\u001b[32m[1114 11:24:45 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:45 @registry.py:135]\u001b[0m group1/block0/conv3 output: [None, 512, None, None]\n",
      "\u001b[32m[1114 11:24:45 @registry.py:127]\u001b[0m group1/block0/convshortcut input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:45 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:45 @registry.py:135]\u001b[0m group1/block0/convshortcut output: [None, 512, None, None]\n",
      "\u001b[32m[1114 11:24:45 @registry.py:127]\u001b[0m group1/block1/conv1 input: [None, 512, None, None]\n",
      "\u001b[32m[1114 11:24:45 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:46 @registry.py:135]\u001b[0m group1/block1/conv1 output: [None, 128, None, None]\n",
      "\u001b[32m[1114 11:24:46 @registry.py:127]\u001b[0m group1/block1/conv2 input: [None, 128, None, None]\n",
      "\u001b[32m[1114 11:24:46 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:46 @registry.py:135]\u001b[0m group1/block1/conv2 output: [None, 128, None, None]\n",
      "\u001b[32m[1114 11:24:46 @registry.py:127]\u001b[0m group1/block1/conv3 input: [None, 128, None, None]\n",
      "\u001b[32m[1114 11:24:46 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:46 @registry.py:135]\u001b[0m group1/block1/conv3 output: [None, 512, None, None]\n",
      "\u001b[32m[1114 11:24:46 @registry.py:127]\u001b[0m group1/block2/conv1 input: [None, 512, None, None]\n",
      "\u001b[32m[1114 11:24:46 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:46 @registry.py:135]\u001b[0m group1/block2/conv1 output: [None, 128, None, None]\n",
      "\u001b[32m[1114 11:24:46 @registry.py:127]\u001b[0m group1/block2/conv2 input: [None, 128, None, None]\n",
      "\u001b[32m[1114 11:24:46 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:46 @registry.py:135]\u001b[0m group1/block2/conv2 output: [None, 128, None, None]\n",
      "\u001b[32m[1114 11:24:46 @registry.py:127]\u001b[0m group1/block2/conv3 input: [None, 128, None, None]\n",
      "\u001b[32m[1114 11:24:46 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:46 @registry.py:135]\u001b[0m group1/block2/conv3 output: [None, 512, None, None]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1114 11:24:46 @registry.py:127]\u001b[0m group1/block3/conv1 input: [None, 512, None, None]\n",
      "\u001b[32m[1114 11:24:46 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:46 @registry.py:135]\u001b[0m group1/block3/conv1 output: [None, 128, None, None]\n",
      "\u001b[32m[1114 11:24:46 @registry.py:127]\u001b[0m group1/block3/conv2 input: [None, 128, None, None]\n",
      "\u001b[32m[1114 11:24:46 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:46 @registry.py:135]\u001b[0m group1/block3/conv2 output: [None, 128, None, None]\n",
      "\u001b[32m[1114 11:24:46 @registry.py:127]\u001b[0m group1/block3/conv3 input: [None, 128, None, None]\n",
      "\u001b[32m[1114 11:24:46 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:46 @registry.py:135]\u001b[0m group1/block3/conv3 output: [None, 512, None, None]\n",
      "\u001b[32m[1114 11:24:46 @registry.py:127]\u001b[0m group2/block0/conv1 input: [None, 512, None, None]\n",
      "\u001b[32m[1114 11:24:46 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:46 @registry.py:135]\u001b[0m group2/block0/conv1 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:46 @registry.py:127]\u001b[0m group2/block0/conv2 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:46 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:46 @registry.py:135]\u001b[0m group2/block0/conv2 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:46 @registry.py:127]\u001b[0m group2/block0/conv3 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:46 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:46 @registry.py:135]\u001b[0m group2/block0/conv3 output: [None, 1024, None, None]\n",
      "\u001b[32m[1114 11:24:46 @registry.py:127]\u001b[0m group2/block0/convshortcut input: [None, 512, None, None]\n",
      "\u001b[32m[1114 11:24:46 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:46 @registry.py:135]\u001b[0m group2/block0/convshortcut output: [None, 1024, None, None]\n",
      "\u001b[32m[1114 11:24:46 @registry.py:127]\u001b[0m group2/block1/conv1 input: [None, 1024, None, None]\n",
      "\u001b[32m[1114 11:24:46 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:46 @registry.py:135]\u001b[0m group2/block1/conv1 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:46 @registry.py:127]\u001b[0m group2/block1/conv2 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:46 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:46 @registry.py:135]\u001b[0m group2/block1/conv2 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:46 @registry.py:127]\u001b[0m group2/block1/conv3 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:46 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:46 @registry.py:135]\u001b[0m group2/block1/conv3 output: [None, 1024, None, None]\n",
      "\u001b[32m[1114 11:24:46 @registry.py:127]\u001b[0m group2/block2/conv1 input: [None, 1024, None, None]\n",
      "\u001b[32m[1114 11:24:46 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:47 @registry.py:135]\u001b[0m group2/block2/conv1 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:47 @registry.py:127]\u001b[0m group2/block2/conv2 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:47 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:47 @registry.py:135]\u001b[0m group2/block2/conv2 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:47 @registry.py:127]\u001b[0m group2/block2/conv3 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:47 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:47 @registry.py:135]\u001b[0m group2/block2/conv3 output: [None, 1024, None, None]\n",
      "\u001b[32m[1114 11:24:47 @registry.py:127]\u001b[0m group2/block3/conv1 input: [None, 1024, None, None]\n",
      "\u001b[32m[1114 11:24:47 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:47 @registry.py:135]\u001b[0m group2/block3/conv1 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:47 @registry.py:127]\u001b[0m group2/block3/conv2 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:47 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:47 @registry.py:135]\u001b[0m group2/block3/conv2 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:47 @registry.py:127]\u001b[0m group2/block3/conv3 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:47 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:47 @registry.py:135]\u001b[0m group2/block3/conv3 output: [None, 1024, None, None]\n",
      "\u001b[32m[1114 11:24:47 @registry.py:127]\u001b[0m group2/block4/conv1 input: [None, 1024, None, None]\n",
      "\u001b[32m[1114 11:24:47 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:47 @registry.py:135]\u001b[0m group2/block4/conv1 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:47 @registry.py:127]\u001b[0m group2/block4/conv2 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:47 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:47 @registry.py:135]\u001b[0m group2/block4/conv2 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:47 @registry.py:127]\u001b[0m group2/block4/conv3 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:47 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:47 @registry.py:135]\u001b[0m group2/block4/conv3 output: [None, 1024, None, None]\n",
      "\u001b[32m[1114 11:24:47 @registry.py:127]\u001b[0m group2/block5/conv1 input: [None, 1024, None, None]\n",
      "\u001b[32m[1114 11:24:47 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:47 @registry.py:135]\u001b[0m group2/block5/conv1 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:47 @registry.py:127]\u001b[0m group2/block5/conv2 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:47 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:47 @registry.py:135]\u001b[0m group2/block5/conv2 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:47 @registry.py:127]\u001b[0m group2/block5/conv3 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:47 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:47 @registry.py:135]\u001b[0m group2/block5/conv3 output: [None, 1024, None, None]\n",
      "\u001b[32m[1114 11:24:47 @registry.py:127]\u001b[0m group3/block0/conv1 input: [None, 1024, None, None]\n",
      "\u001b[32m[1114 11:24:47 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:47 @registry.py:135]\u001b[0m group3/block0/conv1 output: [None, 512, None, None]\n",
      "\u001b[32m[1114 11:24:47 @registry.py:127]\u001b[0m group3/block0/conv2 input: [None, 512, None, None]\n",
      "\u001b[32m[1114 11:24:47 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:47 @registry.py:135]\u001b[0m group3/block0/conv2 output: [None, 512, None, None]\n",
      "\u001b[32m[1114 11:24:47 @registry.py:127]\u001b[0m group3/block0/conv3 input: [None, 512, None, None]\n",
      "\u001b[32m[1114 11:24:47 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:47 @registry.py:135]\u001b[0m group3/block0/conv3 output: [None, 2048, None, None]\n",
      "\u001b[32m[1114 11:24:47 @registry.py:127]\u001b[0m group3/block0/convshortcut input: [None, 1024, None, None]\n",
      "\u001b[32m[1114 11:24:47 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:47 @registry.py:135]\u001b[0m group3/block0/convshortcut output: [None, 2048, None, None]\n",
      "\u001b[32m[1114 11:24:47 @registry.py:127]\u001b[0m group3/block1/conv1 input: [None, 2048, None, None]\n",
      "\u001b[32m[1114 11:24:47 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1114 11:24:47 @registry.py:135]\u001b[0m group3/block1/conv1 output: [None, 512, None, None]\n",
      "\u001b[32m[1114 11:24:47 @registry.py:127]\u001b[0m group3/block1/conv2 input: [None, 512, None, None]\n",
      "\u001b[32m[1114 11:24:48 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:48 @registry.py:135]\u001b[0m group3/block1/conv2 output: [None, 512, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:127]\u001b[0m group3/block1/conv3 input: [None, 512, None, None]\n",
      "\u001b[32m[1114 11:24:48 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:48 @registry.py:135]\u001b[0m group3/block1/conv3 output: [None, 2048, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:127]\u001b[0m group3/block2/conv1 input: [None, 2048, None, None]\n",
      "\u001b[32m[1114 11:24:48 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:48 @registry.py:135]\u001b[0m group3/block2/conv1 output: [None, 512, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:127]\u001b[0m group3/block2/conv2 input: [None, 512, None, None]\n",
      "\u001b[32m[1114 11:24:48 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:48 @registry.py:135]\u001b[0m group3/block2/conv2 output: [None, 512, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:127]\u001b[0m group3/block2/conv3 input: [None, 512, None, None]\n",
      "\u001b[32m[1114 11:24:48 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1114 11:24:48 @registry.py:135]\u001b[0m group3/block2/conv3 output: [None, 2048, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:127]\u001b[0m fpn input: [None, 256, None, None],[None, 512, None, None],[None, 1024, None, None],[None, 2048, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:127]\u001b[0m fpn/lateral_1x1_c2 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:135]\u001b[0m fpn/lateral_1x1_c2 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:127]\u001b[0m fpn/lateral_1x1_c3 input: [None, 512, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:135]\u001b[0m fpn/lateral_1x1_c3 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:127]\u001b[0m fpn/lateral_1x1_c4 input: [None, 1024, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:135]\u001b[0m fpn/lateral_1x1_c4 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:127]\u001b[0m fpn/lateral_1x1_c5 input: [None, 2048, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:135]\u001b[0m fpn/lateral_1x1_c5 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:127]\u001b[0m fpn/upsample_lat5 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:135]\u001b[0m fpn/upsample_lat5 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:127]\u001b[0m fpn/upsample_lat4 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:135]\u001b[0m fpn/upsample_lat4 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:127]\u001b[0m fpn/upsample_lat3 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:135]\u001b[0m fpn/upsample_lat3 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:127]\u001b[0m fpn/posthoc_3x3_p2 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:135]\u001b[0m fpn/posthoc_3x3_p2 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:127]\u001b[0m fpn/posthoc_3x3_p3 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:135]\u001b[0m fpn/posthoc_3x3_p3 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:127]\u001b[0m fpn/posthoc_3x3_p4 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:135]\u001b[0m fpn/posthoc_3x3_p4 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:127]\u001b[0m fpn/posthoc_3x3_p5 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:135]\u001b[0m fpn/posthoc_3x3_p5 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:127]\u001b[0m fpn/maxpool_p6 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:135]\u001b[0m fpn/maxpool_p6 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:135]\u001b[0m fpn output: [None, 256, None, None],[None, 256, None, None],[None, 256, None, None],[None, 256, None, None],[None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:127]\u001b[0m rpn input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:127]\u001b[0m rpn/conv0 input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:135]\u001b[0m rpn/conv0 output: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:127]\u001b[0m rpn/class input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:135]\u001b[0m rpn/class output: [None, 3, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:127]\u001b[0m rpn/box input: [None, 256, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:135]\u001b[0m rpn/box output: [None, 12, None, None]\n",
      "\u001b[32m[1114 11:24:48 @registry.py:135]\u001b[0m rpn output: [None, None, None, 3],[None, 12, None, None]\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/MaskRCNN/model/rpn.py:123: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
      "\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/MaskRCNN/model/rpn.py:125: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
      "\n",
      "\u001b[32m[1114 11:24:50 @registry.py:127]\u001b[0m fastrcnn input: [None, 256, 7, 7]\n",
      "\u001b[32m[1114 11:24:50 @registry.py:127]\u001b[0m fastrcnn/fc6 input: [None, 256, 7, 7]\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/models/fc.py:59: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n",
      "\n",
      "\u001b[32m[1114 11:24:50 @registry.py:135]\u001b[0m fastrcnn/fc6 output: [None, 1024]\n",
      "\u001b[32m[1114 11:24:50 @registry.py:127]\u001b[0m fastrcnn/fc7 input: [None, 1024]\n",
      "\u001b[32m[1114 11:24:50 @registry.py:135]\u001b[0m fastrcnn/fc7 output: [None, 1024]\n",
      "\u001b[32m[1114 11:24:50 @registry.py:135]\u001b[0m fastrcnn output: [None, 1024]\n",
      "\u001b[32m[1114 11:24:50 @registry.py:127]\u001b[0m fastrcnn/outputs input: [None, 1024]\n",
      "\u001b[32m[1114 11:24:50 @registry.py:127]\u001b[0m fastrcnn/outputs/class input: [None, 1024]\n",
      "\u001b[32m[1114 11:24:51 @registry.py:135]\u001b[0m fastrcnn/outputs/class output: [None, 81]\n",
      "\u001b[32m[1114 11:24:51 @registry.py:127]\u001b[0m fastrcnn/outputs/box input: [None, 1024]\n",
      "\u001b[32m[1114 11:24:51 @registry.py:135]\u001b[0m fastrcnn/outputs/box output: [None, 324]\n",
      "\u001b[32m[1114 11:24:51 @registry.py:135]\u001b[0m fastrcnn/outputs output: [None, 81],[None, 81, 4]\n",
      "\u001b[32m[1114 11:24:51 @registry.py:127]\u001b[0m maskrcnn input: [None, 256, 14, 14]\n",
      "\u001b[32m[1114 11:24:51 @registry.py:127]\u001b[0m maskrcnn/fcn0 input: [None, 256, 14, 14]\n",
      "\u001b[32m[1114 11:24:51 @registry.py:135]\u001b[0m maskrcnn/fcn0 output: [None, 256, 14, 14]\n",
      "\u001b[32m[1114 11:24:51 @registry.py:127]\u001b[0m maskrcnn/fcn1 input: [None, 256, 14, 14]\n",
      "\u001b[32m[1114 11:24:51 @registry.py:135]\u001b[0m maskrcnn/fcn1 output: [None, 256, 14, 14]\n",
      "\u001b[32m[1114 11:24:51 @registry.py:127]\u001b[0m maskrcnn/fcn2 input: [None, 256, 14, 14]\n",
      "\u001b[32m[1114 11:24:51 @registry.py:135]\u001b[0m maskrcnn/fcn2 output: [None, 256, 14, 14]\n",
      "\u001b[32m[1114 11:24:51 @registry.py:127]\u001b[0m maskrcnn/fcn3 input: [None, 256, 14, 14]\n",
      "\u001b[32m[1114 11:24:51 @registry.py:135]\u001b[0m maskrcnn/fcn3 output: [None, 256, 14, 14]\n",
      "\u001b[32m[1114 11:24:51 @registry.py:127]\u001b[0m maskrcnn/deconv input: [None, 256, 14, 14]\n",
      "\u001b[32m[1114 11:24:51 @registry.py:135]\u001b[0m maskrcnn/deconv output: [None, 256, 28, 28]\n",
      "\u001b[32m[1114 11:24:51 @registry.py:127]\u001b[0m maskrcnn/conv input: [None, 256, 28, 28]\n",
      "\u001b[32m[1114 11:24:51 @registry.py:135]\u001b[0m maskrcnn/conv output: [None, 80, 28, 28]\n",
      "\u001b[32m[1114 11:24:51 @registry.py:135]\u001b[0m maskrcnn output: [None, 80, 28, 28]\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/MaskRCNN/model_box.py:195: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/MaskRCNN/model/mask_head.py:36: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name mask_truth|pred is illegal; using mask_truth_pred instead.\n",
      "\u001b[32m[1114 11:24:52 @regularize.py:97]\u001b[0m regularize_cost() found 63 variables to regularize.\n",
      "\u001b[32m[1114 11:24:52 @regularize.py:22]\u001b[0m The following tensors will be regularized: group1/block0/conv1/W:0, group1/block0/conv2/W:0, group1/block0/conv3/W:0, group1/block0/convshortcut/W:0, group1/block1/conv1/W:0, group1/block1/conv2/W:0, group1/block1/conv3/W:0, group1/block2/conv1/W:0, group1/block2/conv2/W:0, group1/block2/conv3/W:0, group1/block3/conv1/W:0, group1/block3/conv2/W:0, group1/block3/conv3/W:0, group2/block0/conv1/W:0, group2/block0/conv2/W:0, group2/block0/conv3/W:0, group2/block0/convshortcut/W:0, group2/block1/conv1/W:0, group2/block1/conv2/W:0, group2/block1/conv3/W:0, group2/block2/conv1/W:0, group2/block2/conv2/W:0, group2/block2/conv3/W:0, group2/block3/conv1/W:0, group2/block3/conv2/W:0, group2/block3/conv3/W:0, group2/block4/conv1/W:0, group2/block4/conv2/W:0, group2/block4/conv3/W:0, group2/block5/conv1/W:0, group2/block5/conv2/W:0, group2/block5/conv3/W:0, group3/block0/conv1/W:0, group3/block0/conv2/W:0, group3/block0/conv3/W:0, group3/block0/convshortcut/W:0, group3/block1/conv1/W:0, group3/block1/conv2/W:0, group3/block1/conv3/W:0, group3/block2/conv1/W:0, group3/block2/conv2/W:0, group3/block2/conv3/W:0, fpn/lateral_1x1_c2/W:0, fpn/lateral_1x1_c3/W:0, fpn/lateral_1x1_c4/W:0, fpn/lateral_1x1_c5/W:0, fpn/posthoc_3x3_p2/W:0, fpn/posthoc_3x3_p3/W:0, fpn/posthoc_3x3_p4/W:0, fpn/posthoc_3x3_p5/W:0, rpn/conv0/W:0, rpn/class/W:0, rpn/box/W:0, fastrcnn/fc6/W:0, fastrcnn/fc7/W:0, fastrcnn/outputs/class/W:0, fastrcnn/outputs/box/W:0, maskrcnn/fcn0/W:0, maskrcnn/fcn1/W:0, maskrcnn/fcn2/W:0, maskrcnn/fcn3/W:0, maskrcnn/deconv/W:0, maskrcnn/conv/W:0\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/MaskRCNN/model/generalized_rcnn.py:93: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1114 11:24:57 @monitor.py:259]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m logger directory was not set. Ignore TFEventWriter.\n",
      "\u001b[32m[1114 11:24:57 @monitor.py:300]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m logger directory was not set. Ignore JSONWriter.\n",
      "\u001b[32m[1114 11:24:57 @model_utils.py:66]\u001b[0m \u001b[36mTrainable Variables: \n",
      "\u001b[0mname                                   shape                    dim\n",
      "-------------------------------------  ------------------  --------\n",
      "group1/block0/conv1/W:0                [1, 1, 256, 128]       32768\n",
      "group1/block0/conv1/bn/gamma:0         [128]                    128\n",
      "group1/block0/conv1/bn/beta:0          [128]                    128\n",
      "group1/block0/conv2/W:0                [3, 3, 128, 128]      147456\n",
      "group1/block0/conv2/bn/gamma:0         [128]                    128\n",
      "group1/block0/conv2/bn/beta:0          [128]                    128\n",
      "group1/block0/conv3/W:0                [1, 1, 128, 512]       65536\n",
      "group1/block0/conv3/bn/gamma:0         [512]                    512\n",
      "group1/block0/conv3/bn/beta:0          [512]                    512\n",
      "group1/block0/convshortcut/W:0         [1, 1, 256, 512]      131072\n",
      "group1/block0/convshortcut/bn/gamma:0  [512]                    512\n",
      "group1/block0/convshortcut/bn/beta:0   [512]                    512\n",
      "group1/block1/conv1/W:0                [1, 1, 512, 128]       65536\n",
      "group1/block1/conv1/bn/gamma:0         [128]                    128\n",
      "group1/block1/conv1/bn/beta:0          [128]                    128\n",
      "group1/block1/conv2/W:0                [3, 3, 128, 128]      147456\n",
      "group1/block1/conv2/bn/gamma:0         [128]                    128\n",
      "group1/block1/conv2/bn/beta:0          [128]                    128\n",
      "group1/block1/conv3/W:0                [1, 1, 128, 512]       65536\n",
      "group1/block1/conv3/bn/gamma:0         [512]                    512\n",
      "group1/block1/conv3/bn/beta:0          [512]                    512\n",
      "group1/block2/conv1/W:0                [1, 1, 512, 128]       65536\n",
      "group1/block2/conv1/bn/gamma:0         [128]                    128\n",
      "group1/block2/conv1/bn/beta:0          [128]                    128\n",
      "group1/block2/conv2/W:0                [3, 3, 128, 128]      147456\n",
      "group1/block2/conv2/bn/gamma:0         [128]                    128\n",
      "group1/block2/conv2/bn/beta:0          [128]                    128\n",
      "group1/block2/conv3/W:0                [1, 1, 128, 512]       65536\n",
      "group1/block2/conv3/bn/gamma:0         [512]                    512\n",
      "group1/block2/conv3/bn/beta:0          [512]                    512\n",
      "group1/block3/conv1/W:0                [1, 1, 512, 128]       65536\n",
      "group1/block3/conv1/bn/gamma:0         [128]                    128\n",
      "group1/block3/conv1/bn/beta:0          [128]                    128\n",
      "group1/block3/conv2/W:0                [3, 3, 128, 128]      147456\n",
      "group1/block3/conv2/bn/gamma:0         [128]                    128\n",
      "group1/block3/conv2/bn/beta:0          [128]                    128\n",
      "group1/block3/conv3/W:0                [1, 1, 128, 512]       65536\n",
      "group1/block3/conv3/bn/gamma:0         [512]                    512\n",
      "group1/block3/conv3/bn/beta:0          [512]                    512\n",
      "group2/block0/conv1/W:0                [1, 1, 512, 256]      131072\n",
      "group2/block0/conv1/bn/gamma:0         [256]                    256\n",
      "group2/block0/conv1/bn/beta:0          [256]                    256\n",
      "group2/block0/conv2/W:0                [3, 3, 256, 256]      589824\n",
      "group2/block0/conv2/bn/gamma:0         [256]                    256\n",
      "group2/block0/conv2/bn/beta:0          [256]                    256\n",
      "group2/block0/conv3/W:0                [1, 1, 256, 1024]     262144\n",
      "group2/block0/conv3/bn/gamma:0         [1024]                  1024\n",
      "group2/block0/conv3/bn/beta:0          [1024]                  1024\n",
      "group2/block0/convshortcut/W:0         [1, 1, 512, 1024]     524288\n",
      "group2/block0/convshortcut/bn/gamma:0  [1024]                  1024\n",
      "group2/block0/convshortcut/bn/beta:0   [1024]                  1024\n",
      "group2/block1/conv1/W:0                [1, 1, 1024, 256]     262144\n",
      "group2/block1/conv1/bn/gamma:0         [256]                    256\n",
      "group2/block1/conv1/bn/beta:0          [256]                    256\n",
      "group2/block1/conv2/W:0                [3, 3, 256, 256]      589824\n",
      "group2/block1/conv2/bn/gamma:0         [256]                    256\n",
      "group2/block1/conv2/bn/beta:0          [256]                    256\n",
      "group2/block1/conv3/W:0                [1, 1, 256, 1024]     262144\n",
      "group2/block1/conv3/bn/gamma:0         [1024]                  1024\n",
      "group2/block1/conv3/bn/beta:0          [1024]                  1024\n",
      "group2/block2/conv1/W:0                [1, 1, 1024, 256]     262144\n",
      "group2/block2/conv1/bn/gamma:0         [256]                    256\n",
      "group2/block2/conv1/bn/beta:0          [256]                    256\n",
      "group2/block2/conv2/W:0                [3, 3, 256, 256]      589824\n",
      "group2/block2/conv2/bn/gamma:0         [256]                    256\n",
      "group2/block2/conv2/bn/beta:0          [256]                    256\n",
      "group2/block2/conv3/W:0                [1, 1, 256, 1024]     262144\n",
      "group2/block2/conv3/bn/gamma:0         [1024]                  1024\n",
      "group2/block2/conv3/bn/beta:0          [1024]                  1024\n",
      "group2/block3/conv1/W:0                [1, 1, 1024, 256]     262144\n",
      "group2/block3/conv1/bn/gamma:0         [256]                    256\n",
      "group2/block3/conv1/bn/beta:0          [256]                    256\n",
      "group2/block3/conv2/W:0                [3, 3, 256, 256]      589824\n",
      "group2/block3/conv2/bn/gamma:0         [256]                    256\n",
      "group2/block3/conv2/bn/beta:0          [256]                    256\n",
      "group2/block3/conv3/W:0                [1, 1, 256, 1024]     262144\n",
      "group2/block3/conv3/bn/gamma:0         [1024]                  1024\n",
      "group2/block3/conv3/bn/beta:0          [1024]                  1024\n",
      "group2/block4/conv1/W:0                [1, 1, 1024, 256]     262144\n",
      "group2/block4/conv1/bn/gamma:0         [256]                    256\n",
      "group2/block4/conv1/bn/beta:0          [256]                    256\n",
      "group2/block4/conv2/W:0                [3, 3, 256, 256]      589824\n",
      "group2/block4/conv2/bn/gamma:0         [256]                    256\n",
      "group2/block4/conv2/bn/beta:0          [256]                    256\n",
      "group2/block4/conv3/W:0                [1, 1, 256, 1024]     262144\n",
      "group2/block4/conv3/bn/gamma:0         [1024]                  1024\n",
      "group2/block4/conv3/bn/beta:0          [1024]                  1024\n",
      "group2/block5/conv1/W:0                [1, 1, 1024, 256]     262144\n",
      "group2/block5/conv1/bn/gamma:0         [256]                    256\n",
      "group2/block5/conv1/bn/beta:0          [256]                    256\n",
      "group2/block5/conv2/W:0                [3, 3, 256, 256]      589824\n",
      "group2/block5/conv2/bn/gamma:0         [256]                    256\n",
      "group2/block5/conv2/bn/beta:0          [256]                    256\n",
      "group2/block5/conv3/W:0                [1, 1, 256, 1024]     262144\n",
      "group2/block5/conv3/bn/gamma:0         [1024]                  1024\n",
      "group2/block5/conv3/bn/beta:0          [1024]                  1024\n",
      "group3/block0/conv1/W:0                [1, 1, 1024, 512]     524288\n",
      "group3/block0/conv1/bn/gamma:0         [512]                    512\n",
      "group3/block0/conv1/bn/beta:0          [512]                    512\n",
      "group3/block0/conv2/W:0                [3, 3, 512, 512]     2359296\n",
      "group3/block0/conv2/bn/gamma:0         [512]                    512\n",
      "group3/block0/conv2/bn/beta:0          [512]                    512\n",
      "group3/block0/conv3/W:0                [1, 1, 512, 2048]    1048576\n",
      "group3/block0/conv3/bn/gamma:0         [2048]                  2048\n",
      "group3/block0/conv3/bn/beta:0          [2048]                  2048\n",
      "group3/block0/convshortcut/W:0         [1, 1, 1024, 2048]   2097152\n",
      "group3/block0/convshortcut/bn/gamma:0  [2048]                  2048\n",
      "group3/block0/convshortcut/bn/beta:0   [2048]                  2048\n",
      "group3/block1/conv1/W:0                [1, 1, 2048, 512]    1048576\n",
      "group3/block1/conv1/bn/gamma:0         [512]                    512\n",
      "group3/block1/conv1/bn/beta:0          [512]                    512\n",
      "group3/block1/conv2/W:0                [3, 3, 512, 512]     2359296\n",
      "group3/block1/conv2/bn/gamma:0         [512]                    512\n",
      "group3/block1/conv2/bn/beta:0          [512]                    512\n",
      "group3/block1/conv3/W:0                [1, 1, 512, 2048]    1048576\n",
      "group3/block1/conv3/bn/gamma:0         [2048]                  2048\n",
      "group3/block1/conv3/bn/beta:0          [2048]                  2048\n",
      "group3/block2/conv1/W:0                [1, 1, 2048, 512]    1048576\n",
      "group3/block2/conv1/bn/gamma:0         [512]                    512\n",
      "group3/block2/conv1/bn/beta:0          [512]                    512\n",
      "group3/block2/conv2/W:0                [3, 3, 512, 512]     2359296\n",
      "group3/block2/conv2/bn/gamma:0         [512]                    512\n",
      "group3/block2/conv2/bn/beta:0          [512]                    512\n",
      "group3/block2/conv3/W:0                [1, 1, 512, 2048]    1048576\n",
      "group3/block2/conv3/bn/gamma:0         [2048]                  2048\n",
      "group3/block2/conv3/bn/beta:0          [2048]                  2048\n",
      "fpn/lateral_1x1_c2/W:0                 [1, 1, 256, 256]       65536\n",
      "fpn/lateral_1x1_c2/b:0                 [256]                    256\n",
      "fpn/lateral_1x1_c3/W:0                 [1, 1, 512, 256]      131072\n",
      "fpn/lateral_1x1_c3/b:0                 [256]                    256\n",
      "fpn/lateral_1x1_c4/W:0                 [1, 1, 1024, 256]     262144\n",
      "fpn/lateral_1x1_c4/b:0                 [256]                    256\n",
      "fpn/lateral_1x1_c5/W:0                 [1, 1, 2048, 256]     524288\n",
      "fpn/lateral_1x1_c5/b:0                 [256]                    256\n",
      "fpn/posthoc_3x3_p2/W:0                 [3, 3, 256, 256]      589824\n",
      "fpn/posthoc_3x3_p2/b:0                 [256]                    256\n",
      "fpn/posthoc_3x3_p3/W:0                 [3, 3, 256, 256]      589824\n",
      "fpn/posthoc_3x3_p3/b:0                 [256]                    256\n",
      "fpn/posthoc_3x3_p4/W:0                 [3, 3, 256, 256]      589824\n",
      "fpn/posthoc_3x3_p4/b:0                 [256]                    256\n",
      "fpn/posthoc_3x3_p5/W:0                 [3, 3, 256, 256]      589824\n",
      "fpn/posthoc_3x3_p5/b:0                 [256]                    256\n",
      "rpn/conv0/W:0                          [3, 3, 256, 256]      589824\n",
      "rpn/conv0/b:0                          [256]                    256\n",
      "rpn/class/W:0                          [1, 1, 256, 3]           768\n",
      "rpn/class/b:0                          [3]                        3\n",
      "rpn/box/W:0                            [1, 1, 256, 12]         3072\n",
      "rpn/box/b:0                            [12]                      12\n",
      "fastrcnn/fc6/W:0                       [12544, 1024]       12845056\n",
      "fastrcnn/fc6/b:0                       [1024]                  1024\n",
      "fastrcnn/fc7/W:0                       [1024, 1024]         1048576\n",
      "fastrcnn/fc7/b:0                       [1024]                  1024\n",
      "fastrcnn/outputs/class/W:0             [1024, 81]             82944\n",
      "fastrcnn/outputs/class/b:0             [81]                      81\n",
      "fastrcnn/outputs/box/W:0               [1024, 324]           331776\n",
      "fastrcnn/outputs/box/b:0               [324]                    324\n",
      "maskrcnn/fcn0/W:0                      [3, 3, 256, 256]      589824\n",
      "maskrcnn/fcn0/b:0                      [256]                    256\n",
      "maskrcnn/fcn1/W:0                      [3, 3, 256, 256]      589824\n",
      "maskrcnn/fcn1/b:0                      [256]                    256\n",
      "maskrcnn/fcn2/W:0                      [3, 3, 256, 256]      589824\n",
      "maskrcnn/fcn2/b:0                      [256]                    256\n",
      "maskrcnn/fcn3/W:0                      [3, 3, 256, 256]      589824\n",
      "maskrcnn/fcn3/b:0                      [256]                    256\n",
      "maskrcnn/deconv/W:0                    [2, 2, 256, 256]      262144\n",
      "maskrcnn/deconv/b:0                    [256]                    256\n",
      "maskrcnn/conv/W:0                      [1, 1, 256, 80]        20480\n",
      "maskrcnn/conv/b:0                      [80]                      80\u001b[36m\n",
      "Total #vars=168, #params=44175092, size=168.51MB\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1114 11:24:57 @base.py:210]\u001b[0m Setup callbacks graph ...\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/callbacks/graph.py:56: The name tf.train.SessionRunArgs is deprecated. Please use tf.estimator.SessionRunArgs instead.\n",
      "\n",
      "\u001b[32m[1114 11:24:57 @argtools.py:148]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m \"import prctl\" failed! Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[1114 11:24:57 @summary.py:48]\u001b[0m [MovingAverageSummary] 27 operations in collection 'MOVING_SUMMARY_OPS' will be run with session hooks.\n",
      "\u001b[32m[1114 11:24:57 @summary.py:95]\u001b[0m Summarizing collection 'summaries' of size 30.\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/callbacks/summary.py:96: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "\u001b[32m[1114 11:24:57 @base.py:231]\u001b[0m Creating the session ...\n",
      "\u001b[32m[1114 11:25:07 @base.py:237]\u001b[0m Initializing the session ...\n",
      "\u001b[32m[1114 11:25:07 @sessinit.py:206]\u001b[0m Variables to restore from dict: group3/block2/conv2/bn/gamma:0, group0/block2/conv1/bn/variance/EMA:0, group2/block4/conv1/bn/variance/EMA:0, group1/block0/conv3/bn/gamma:0, group2/block2/conv2/W:0, group3/block1/conv2/bn/mean/EMA:0, group0/block0/conv1/bn/mean/EMA:0, group2/block5/conv2/bn/mean/EMA:0, group2/block3/conv2/W:0, group0/block1/conv3/bn/gamma:0, group3/block2/conv1/W:0, group2/block4/conv3/bn/gamma:0, group0/block1/conv1/W:0, group2/block0/conv1/bn/variance/EMA:0, group0/block1/conv3/bn/beta:0, group1/block3/conv1/bn/mean/EMA:0, group0/block2/conv1/W:0, group2/block0/conv3/bn/mean/EMA:0, group0/block0/convshortcut/bn/variance/EMA:0, group3/block0/conv1/bn/mean/EMA:0, group0/block2/conv3/bn/gamma:0, group0/block2/conv2/bn/gamma:0, group2/block0/convshortcut/bn/gamma:0, group2/block2/conv1/bn/mean/EMA:0, group3/block1/conv1/W:0, group1/block2/conv1/bn/mean/EMA:0, group3/block2/conv2/bn/mean/EMA:0, group1/block0/conv1/bn/beta:0, group0/block0/convshortcut/W:0, group0/block2/conv2/bn/mean/EMA:0, group3/block1/conv1/bn/gamma:0, group0/block1/conv1/bn/variance/EMA:0, group2/block1/conv1/bn/variance/EMA:0, group2/block4/conv3/W:0, group2/block4/conv2/bn/gamma:0, group2/block1/conv1/bn/beta:0, group1/block1/conv3/bn/gamma:0, group2/block4/conv2/W:0, group1/block1/conv3/bn/mean/EMA:0, group2/block3/conv2/bn/variance/EMA:0, group2/block1/conv1/W:0, group0/block0/conv2/bn/beta:0, group3/block1/conv3/bn/gamma:0, group1/block1/conv3/bn/variance/EMA:0, group1/block3/conv1/bn/gamma:0, group3/block2/conv1/bn/variance/EMA:0, group1/block3/conv2/bn/gamma:0, group2/block0/conv2/bn/variance/EMA:0, group2/block3/conv3/bn/mean/EMA:0, group2/block5/conv1/bn/mean/EMA:0, conv0/W:0, group2/block0/conv3/bn/beta:0, group1/block2/conv2/W:0, group2/block5/conv3/bn/mean/EMA:0, group3/block2/conv1/bn/beta:0, group0/block2/conv1/bn/gamma:0, group1/block3/conv3/bn/gamma:0, group2/block3/conv2/bn/beta:0, group0/block1/conv1/bn/mean/EMA:0, group2/block2/conv3/W:0, group1/block2/conv2/bn/beta:0, group1/block3/conv3/bn/mean/EMA:0, group2/block0/conv2/W:0, group3/block2/conv1/bn/gamma:0, group0/block0/conv1/bn/gamma:0, group2/block3/conv1/bn/beta:0, group2/block0/conv1/bn/mean/EMA:0, group0/block2/conv1/bn/beta:0, group2/block2/conv3/bn/beta:0, group1/block1/conv1/W:0, group0/block1/conv2/bn/mean/EMA:0, group0/block2/conv2/bn/variance/EMA:0, group3/block2/conv3/bn/mean/EMA:0, group2/block3/conv1/bn/gamma:0, group1/block1/conv1/bn/gamma:0, group0/block0/conv1/bn/variance/EMA:0, group0/block1/conv2/W:0, group0/block2/conv3/bn/mean/EMA:0, group2/block4/conv1/bn/mean/EMA:0, group2/block0/conv2/bn/gamma:0, group3/block0/conv2/bn/beta:0, group1/block3/conv2/bn/mean/EMA:0, group0/block2/conv3/W:0, group1/block2/conv2/bn/gamma:0, group0/block0/conv1/bn/beta:0, group3/block2/conv3/bn/variance/EMA:0, group1/block0/conv1/W:0, group2/block4/conv1/bn/gamma:0, group2/block4/conv2/bn/variance/EMA:0, group2/block2/conv2/bn/variance/EMA:0, group2/block3/conv3/bn/beta:0, group0/block0/conv3/bn/beta:0, group3/block0/conv2/bn/mean/EMA:0, group1/block1/conv1/bn/beta:0, group3/block1/conv1/bn/mean/EMA:0, group2/block3/conv3/bn/variance/EMA:0, group3/block0/conv3/bn/mean/EMA:0, group2/block0/conv1/W:0, group2/block5/conv2/bn/beta:0, group2/block1/conv1/bn/gamma:0, group1/block3/conv2/W:0, group2/block0/convshortcut/W:0, group1/block3/conv1/W:0, group0/block0/conv2/bn/mean/EMA:0, group3/block0/convshortcut/bn/beta:0, group2/block1/conv2/W:0, group2/block2/conv2/bn/gamma:0, group3/block1/conv2/bn/beta:0, group1/block2/conv2/bn/variance/EMA:0, group2/block5/conv1/bn/variance/EMA:0, group0/block1/conv3/bn/mean/EMA:0, group3/block0/convshortcut/bn/mean/EMA:0, group3/block2/conv2/bn/beta:0, group3/block1/conv2/W:0, group1/block0/convshortcut/bn/beta:0, group3/block0/convshortcut/bn/variance/EMA:0, group3/block1/conv1/bn/beta:0, group2/block2/conv1/bn/beta:0, group2/block0/convshortcut/bn/beta:0, group3/block2/conv1/bn/mean/EMA:0, group2/block4/conv1/W:0, group0/block0/conv3/W:0, group3/block0/conv1/bn/variance/EMA:0, group0/block1/conv3/W:0, group3/block1/conv1/bn/variance/EMA:0, group1/block0/convshortcut/bn/mean/EMA:0, group2/block1/conv3/bn/gamma:0, group2/block4/conv3/bn/variance/EMA:0, group1/block2/conv1/bn/gamma:0, group2/block0/conv3/W:0, group1/block1/conv2/bn/gamma:0, group1/block0/convshortcut/bn/variance/EMA:0, group1/block3/conv1/bn/beta:0, group2/block3/conv1/bn/mean/EMA:0, group1/block2/conv3/bn/variance/EMA:0, group2/block5/conv3/bn/gamma:0, group2/block2/conv1/bn/variance/EMA:0, group2/block5/conv3/bn/beta:0, group0/block1/conv2/bn/beta:0, group1/block1/conv3/bn/beta:0, group3/block1/conv3/bn/mean/EMA:0, group1/block0/convshortcut/bn/gamma:0, group2/block1/conv3/bn/beta:0, group2/block5/conv2/bn/gamma:0, group2/block5/conv1/W:0, group3/block0/conv2/W:0, group0/block0/convshortcut/bn/beta:0, group2/block3/conv2/bn/mean/EMA:0, group3/block0/conv1/W:0, group1/block2/conv1/bn/variance/EMA:0, group0/block1/conv2/bn/gamma:0, conv0/bn/gamma:0, group0/block0/convshortcut/bn/gamma:0, group2/block5/conv1/bn/beta:0, group2/block1/conv3/bn/mean/EMA:0, group1/block1/conv2/bn/variance/EMA:0, group3/block1/conv3/bn/variance/EMA:0, group0/block0/conv3/bn/variance/EMA:0, group3/block0/conv1/bn/beta:0, group1/block0/conv3/bn/beta:0, group3/block1/conv2/bn/gamma:0, group0/block0/conv2/bn/variance/EMA:0, conv0/bn/beta:0, group0/block0/conv1/W:0, group1/block2/conv1/W:0, group1/block1/conv3/W:0, group3/block2/conv3/bn/beta:0, group1/block3/conv1/bn/variance/EMA:0, group2/block1/conv2/bn/mean/EMA:0, group0/block2/conv2/W:0, group1/block0/conv3/bn/variance/EMA:0, group1/block2/conv3/bn/beta:0, group2/block2/conv1/W:0, group2/block2/conv3/bn/variance/EMA:0, group2/block0/convshortcut/bn/variance/EMA:0, conv0/bn/variance/EMA:0, group1/block1/conv1/bn/mean/EMA:0, group2/block2/conv2/bn/mean/EMA:0, group3/block0/conv1/bn/gamma:0, group2/block1/conv3/bn/variance/EMA:0, group3/block1/conv3/bn/beta:0, group3/block1/conv2/bn/variance/EMA:0, group2/block0/conv2/bn/beta:0, group2/block0/conv3/bn/gamma:0, group1/block3/conv3/W:0, group1/block0/conv2/bn/mean/EMA:0, group2/block0/convshortcut/bn/mean/EMA:0, group2/block5/conv2/bn/variance/EMA:0, group3/block0/conv2/bn/gamma:0, group1/block0/conv3/bn/mean/EMA:0, group2/block3/conv3/W:0, group2/block5/conv2/W:0, group1/block2/conv1/bn/beta:0, group2/block5/conv3/bn/variance/EMA:0, group3/block2/conv3/W:0, group1/block0/conv1/bn/gamma:0, group2/block3/conv1/bn/variance/EMA:0, conv0/bn/mean/EMA:0, group1/block3/conv3/bn/variance/EMA:0, group2/block4/conv3/bn/beta:0, group2/block5/conv1/bn/gamma:0, group0/block2/conv3/bn/beta:0, group3/block1/conv3/W:0, group1/block2/conv3/W:0, group0/block1/conv1/bn/beta:0, group0/block1/conv2/bn/variance/EMA:0, group2/block2/conv3/bn/mean/EMA:0, group1/block2/conv3/bn/mean/EMA:0, group3/block0/convshortcut/W:0, group1/block0/conv3/W:0, group1/block2/conv3/bn/gamma:0, group3/block0/convshortcut/bn/gamma:0, group1/block3/conv2/bn/variance/EMA:0, group1/block1/conv1/bn/variance/EMA:0, group2/block0/conv1/bn/gamma:0, group0/block1/conv1/bn/gamma:0, group3/block0/conv3/bn/variance/EMA:0, group2/block1/conv1/bn/mean/EMA:0, group1/block0/conv2/bn/variance/EMA:0, group0/block2/conv2/bn/beta:0, group2/block0/conv3/bn/variance/EMA:0, group2/block3/conv3/bn/gamma:0, group1/block2/conv2/bn/mean/EMA:0, group3/block2/conv3/bn/gamma:0, group0/block2/conv3/bn/variance/EMA:0, group0/block0/conv2/bn/gamma:0, group1/block0/conv2/W:0, group2/block4/conv2/bn/beta:0, group0/block0/conv2/W:0, group2/block1/conv3/W:0, group1/block0/convshortcut/W:0, group2/block1/conv2/bn/beta:0, group2/block0/conv1/bn/beta:0, group2/block2/conv2/bn/beta:0, group1/block1/conv2/bn/mean/EMA:0, group2/block4/conv2/bn/mean/EMA:0, group0/block1/conv3/bn/variance/EMA:0, group2/block4/conv3/bn/mean/EMA:0, group1/block0/conv1/bn/variance/EMA:0, group2/block4/conv1/bn/beta:0, group1/block0/conv2/bn/beta:0, group2/block5/conv3/W:0, group0/block2/conv1/bn/mean/EMA:0, group3/block0/conv3/W:0, group2/block2/conv1/bn/gamma:0, group3/block0/conv2/bn/variance/EMA:0, group0/block0/convshortcut/bn/mean/EMA:0, group3/block2/conv2/bn/variance/EMA:0, group1/block3/conv3/bn/beta:0, group0/block0/conv3/bn/gamma:0, group2/block0/conv2/bn/mean/EMA:0, group3/block0/conv3/bn/beta:0, group2/block1/conv2/bn/variance/EMA:0, group1/block0/conv1/bn/mean/EMA:0, group1/block1/conv2/bn/beta:0, group3/block2/conv2/W:0, group2/block1/conv2/bn/gamma:0, group1/block0/conv2/bn/gamma:0, group2/block3/conv1/W:0, group3/block0/conv3/bn/gamma:0, group1/block3/conv2/bn/beta:0, group1/block1/conv2/W:0, group2/block2/conv3/bn/gamma:0, group2/block3/conv2/bn/gamma:0, group0/block0/conv3/bn/mean/EMA:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1114 11:25:07 @sessinit.py:89]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m The following variables are in the graph, but not found in the dict: fastrcnn/fc6/W, fastrcnn/fc6/b, fastrcnn/fc7/W, fastrcnn/fc7/b, fastrcnn/outputs/box/W, fastrcnn/outputs/box/b, fastrcnn/outputs/class/W, fastrcnn/outputs/class/b, fpn/lateral_1x1_c2/W, fpn/lateral_1x1_c2/b, fpn/lateral_1x1_c3/W, fpn/lateral_1x1_c3/b, fpn/lateral_1x1_c4/W, fpn/lateral_1x1_c4/b, fpn/lateral_1x1_c5/W, fpn/lateral_1x1_c5/b, fpn/posthoc_3x3_p2/W, fpn/posthoc_3x3_p2/b, fpn/posthoc_3x3_p3/W, fpn/posthoc_3x3_p3/b, fpn/posthoc_3x3_p4/W, fpn/posthoc_3x3_p4/b, fpn/posthoc_3x3_p5/W, fpn/posthoc_3x3_p5/b, global_step, learning_rate, maskrcnn/conv/W, maskrcnn/conv/b, maskrcnn/deconv/W, maskrcnn/deconv/b, maskrcnn/fcn0/W, maskrcnn/fcn0/b, maskrcnn/fcn1/W, maskrcnn/fcn1/b, maskrcnn/fcn2/W, maskrcnn/fcn2/b, maskrcnn/fcn3/W, maskrcnn/fcn3/b, rpn/box/W, rpn/box/b, rpn/class/W, rpn/class/b, rpn/conv0/W, rpn/conv0/b\n",
      "\u001b[32m[1114 11:25:07 @sessinit.py:89]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m The following variables are in the dict, but not found in the graph: linear/W, linear/b\n",
      "\u001b[32m[1114 11:25:07 @sessinit.py:219]\u001b[0m Restoring 265 variables from dict ...\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/tfutils/varmanip.py:108: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[32m[1114 11:25:11 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block0/convshortcut/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1114 11:25:11 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block1/conv2/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1114 11:25:13 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block2/conv3/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1114 11:25:14 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block0/conv2/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1114 11:25:14 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable conv0/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1114 11:25:14 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block0/conv1/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1114 11:25:16 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block2/conv1/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1114 11:25:18 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block0/conv3/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1114 11:25:21 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block1/conv3/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1114 11:25:22 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block2/conv2/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1114 11:25:26 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block1/conv1/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1114 11:25:31 @base.py:244]\u001b[0m Graph Finalized.\n",
      "\u001b[32m[1114 11:25:32 @concurrency.py:40]\u001b[0m Starting EnqueueThread QueueInput/input_queue ...\n",
      "\u001b[32m[1114 11:25:32 @base.py:276]\u001b[0m Start Epoch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########|1/1[00:30<00:00, 0.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1114 11:26:02 @base.py:286]\u001b[0m Epoch 1 (global_step 1) finished, time:30.8 seconds.\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m QueueInput/queue_size: 31\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m boxclass_losses/box_loss: 0.038512\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m boxclass_losses/label_loss: 4.4967\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m boxclass_losses/label_metrics/accuracy: 0\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m boxclass_losses/label_metrics/false_negative: 0\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m boxclass_losses/label_metrics/fg_accuracy: 0\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m boxclass_losses/num_fg_label: 13\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m learning_rate: 0.003\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m maskrcnn_loss/accuracy: 0.52463\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m maskrcnn_loss/fg_pixel_ratio: 0.53876\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m maskrcnn_loss/maskrcnn_loss: 1.2385\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m maskrcnn_loss/pos_accuracy: 0.30759\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level2: 453\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level3: 39\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level4: 14\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level5: 6\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level2: 3\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level3: 3\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level4: 3\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level5: 4\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m rpn_losses/box_loss: 0.036621\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m rpn_losses/label_loss: 0.6849\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m sample_fast_rcnn_targets/num_bg: 499\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m sample_fast_rcnn_targets/num_fg: 13\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m sample_fast_rcnn_targets/proposal_metrics/best_iou_per_gt: 0.4148\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m sample_fast_rcnn_targets/proposal_metrics/recall_iou0.3: 0.5\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m sample_fast_rcnn_targets/proposal_metrics/recall_iou0.5: 0.5\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m total_cost: 7.1199\n",
      "\u001b[32m[1114 11:26:02 @monitor.py:469]\u001b[0m wd_cost: 0.62458\n",
      "\u001b[32m[1114 11:26:02 @base.py:290]\u001b[0m Training has finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1114 11:26:03 @input_source.py:178]\u001b[0m EnqueueThread QueueInput/input_queue Exited.\n"
     ]
    }
   ],
   "source": [
    "%%capture cap_out --no-stderr\n",
    "launch_train_with_config(traincfg, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as you followed the convention of `tensor_[name]_forward` and `tensor_[name]_backward` the two cells below will split the output, and convert the tensors from strings to numeric lists. Note that it's generally a bad idea to print out these lists in the notebook, as they can be extremely large, and cause the notebook to crash. Instead, they can be written to an output file. \n",
    "\n",
    "Note that these two paragraphs can take a few minutes to run, as the tensors they parse can be quite large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:03<00:00, 24.76s/it]\n"
     ]
    }
   ],
   "source": [
    "forward_tensors = {i.split(\"_forward \")[0] : \\\n",
    "                   literal_eval(re.sub(\"\\s+\", \", \", i.split(\"_forward \")[1])) \\\n",
    "                   for i in tqdm.tqdm([j for j in \\\n",
    "                                       cap_out.stdout.split(\"[runtime_tensor] tensor_\")[1:] \\\n",
    "                                       if \"_forward \" in j])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [04:56<00:00,  1.77s/it]\n"
     ]
    }
   ],
   "source": [
    "backward_tensors = {i.split(\"_backward \")[0] : \\\n",
    "                    literal_eval(re.sub(\"\\s+\", \", \", i.split(\"_backward \")[1])) \\\n",
    "                    for i in tqdm.tqdm([j for j in \\\n",
    "                                        cap_out.stdout.split(\"[runtime_tensor] tensor_\")[1:] \\\n",
    "                                        if \"_backward \" in j])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some statistics about the tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Tensor Shape\n",
      "p6 : (1, 1, 256, 13, 19)\n",
      "p5 : (1, 1, 256, 25, 37)\n",
      "p4 : (1, 1, 256, 50, 74)\n",
      "p3 : (1, 1, 256, 100, 148)\n",
      "p2 : (1, 1, 256, 200, 296)\n",
      "\n",
      "Backward Tensor Shape\n",
      "rpn/box/b:0 : (1, 12)\n",
      "rpn/box/W:0 : (1, 1, 1, 256, 12)\n",
      "rpn/class/b:0 : (1, 3)\n",
      "rpn/conv0/b:0 : (1, 256)\n",
      "rpn/class/W:0 : (1, 1, 1, 256, 3)\n",
      "fastrcnn/outputs/box/b:0 : (1, 324)\n",
      "fastrcnn/outputs/class/b:0 : (1, 81)\n",
      "fastrcnn/fc7/b:0 : (1, 1024)\n",
      "fastrcnn/fc6/b:0 : (1, 1024)\n",
      "maskrcnn/conv/b:0 : (1, 80)\n",
      "maskrcnn/deconv/b:0 : (1, 256)\n",
      "maskrcnn/fcn3/b:0 : (1, 256)\n",
      "maskrcnn/fcn2/b:0 : (1, 256)\n",
      "maskrcnn/fcn1/b:0 : (1, 256)\n",
      "maskrcnn/fcn0/b:0 : (1, 256)\n",
      "maskrcnn/conv/W:0 : (1, 1, 1, 256, 80)\n",
      "fastrcnn/outputs/class/W:0 : (1, 1024, 81)\n",
      "fpn/posthoc_3x3_p3/b:0 : (1, 256)\n",
      "fpn/posthoc_3x3_p2/b:0 : (1, 256)\n",
      "fpn/posthoc_3x3_p4/b:0 : (1, 256)\n",
      "fpn/posthoc_3x3_p5/b:0 : (1, 256)\n",
      "fpn/lateral_1x1_c2/b:0 : (1, 256)\n",
      "fpn/lateral_1x1_c3/b:0 : (1, 256)\n",
      "fpn/lateral_1x1_c4/b:0 : (1, 256)\n",
      "fpn/lateral_1x1_c5/b:0 : (1, 256)\n",
      "group3/block2/conv3/bn/beta:0 : (1, 2048)\n",
      "group3/block2/conv2/bn/beta:0 : (1, 512)\n",
      "group3/block2/conv3/bn/gamma:0 : (1, 2048)\n",
      "group3/block2/conv2/bn/gamma:0 : (1, 512)\n",
      "group3/block2/conv1/bn/beta:0 : (1, 512)\n",
      "group3/block2/conv1/bn/gamma:0 : (1, 512)\n",
      "group3/block1/conv2/bn/beta:0 : (1, 512)\n",
      "group3/block1/conv3/bn/beta:0 : (1, 2048)\n",
      "group3/block1/conv2/bn/gamma:0 : (1, 512)\n",
      "group3/block1/conv1/bn/beta:0 : (1, 512)\n",
      "group3/block1/conv3/bn/gamma:0 : (1, 2048)\n",
      "group3/block1/conv1/bn/gamma:0 : (1, 512)\n",
      "group3/block0/conv3/bn/beta:0 : (1, 2048)\n",
      "group3/block0/convshortcut/bn/beta:0 : (1, 2048)\n",
      "group3/block0/convshortcut/bn/gamma:0 : (1, 2048)\n",
      "group3/block0/conv2/bn/beta:0 : (1, 512)\n",
      "group3/block0/conv3/bn/gamma:0 : (1, 2048)\n",
      "fpn/lateral_1x1_c2/W:0 : (1, 1, 1, 256, 256)\n",
      "group3/block0/conv2/bn/gamma:0 : (1, 512)\n",
      "group3/block0/conv1/bn/beta:0 : (1, 512)\n",
      "fpn/lateral_1x1_c3/W:0 : (1, 1, 1, 512, 256)\n",
      "group3/block0/conv1/bn/gamma:0 : (1, 512)\n",
      "group2/block5/conv3/bn/beta:0 : (1, 1024)\n",
      "group2/block5/conv2/bn/beta:0 : (1, 256)\n",
      "group2/block5/conv3/bn/gamma:0 : (1, 1024)\n",
      "group2/block5/conv2/bn/gamma:0 : (1, 256)\n",
      "group2/block5/conv1/bn/beta:0 : (1, 256)\n",
      "group2/block5/conv1/bn/gamma:0 : (1, 256)\n",
      "group2/block4/conv3/bn/beta:0 : (1, 1024)\n",
      "group2/block4/conv2/bn/beta:0 : (1, 256)\n",
      "group2/block4/conv3/bn/gamma:0 : (1, 1024)\n",
      "group2/block4/conv2/bn/gamma:0 : (1, 256)\n",
      "group2/block4/conv1/bn/beta:0 : (1, 256)\n",
      "group2/block4/conv1/bn/gamma:0 : (1, 256)\n",
      "group2/block3/conv3/bn/beta:0 : (1, 1024)\n",
      "group2/block3/conv2/bn/beta:0 : (1, 256)\n",
      "group2/block3/conv3/bn/gamma:0 : (1, 1024)\n",
      "group2/block3/conv2/bn/gamma:0 : (1, 256)\n",
      "group2/block3/conv1/bn/beta:0 : (1, 256)\n",
      "group2/block3/conv1/bn/gamma:0 : (1, 256)\n",
      "group2/block2/conv2/bn/beta:0 : (1, 256)\n",
      "group2/block2/conv3/bn/beta:0 : (1, 1024)\n",
      "group2/block2/conv2/bn/gamma:0 : (1, 256)\n",
      "group2/block2/conv3/bn/gamma:0 : (1, 1024)\n",
      "group2/block2/conv1/bn/beta:0 : (1, 256)\n",
      "group2/block2/conv1/bn/gamma:0 : (1, 256)\n",
      "group2/block1/conv2/bn/beta:0 : (1, 256)\n",
      "group2/block1/conv3/bn/beta:0 : (1, 1024)\n",
      "group2/block1/conv2/bn/gamma:0 : (1, 256)\n",
      "group2/block1/conv3/bn/gamma:0 : (1, 1024)\n",
      "group2/block1/conv1/bn/beta:0 : (1, 256)\n",
      "group2/block1/conv1/bn/gamma:0 : (1, 256)\n",
      "group2/block0/conv3/bn/beta:0 : (1, 1024)\n",
      "group2/block0/convshortcut/bn/beta:0 : (1, 1024)\n",
      "group2/block0/conv3/bn/gamma:0 : (1, 1024)\n",
      "group2/block0/conv2/bn/beta:0 : (1, 256)\n",
      "group2/block0/convshortcut/bn/gamma:0 : (1, 1024)\n",
      "group2/block0/conv2/bn/gamma:0 : (1, 256)\n",
      "group2/block0/conv1/bn/beta:0 : (1, 256)\n",
      "group2/block0/conv1/bn/gamma:0 : (1, 256)\n",
      "group1/block3/conv3/bn/beta:0 : (1, 512)\n",
      "fastrcnn/outputs/box/W:0 : (1, 1024, 324)\n",
      "group1/block3/conv3/bn/gamma:0 : (1, 512)\n",
      "group1/block3/conv2/bn/beta:0 : (1, 128)\n",
      "group1/block3/conv2/bn/gamma:0 : (1, 128)\n",
      "group1/block3/conv1/bn/beta:0 : (1, 128)\n",
      "maskrcnn/deconv/W:0 : (1, 2, 2, 256, 256)\n",
      "group1/block1/conv3/bn/gamma:0 : (1, 512)\n",
      "group1/block1/conv2/bn/gamma:0 : (1, 128)\n",
      "group1/block2/conv2/bn/beta:0 : (1, 128)\n",
      "group1/block2/conv1/bn/beta:0 : (1, 128)\n",
      "group1/block1/conv2/bn/beta:0 : (1, 128)\n",
      "group1/block1/conv3/bn/beta:0 : (1, 512)\n",
      "group1/block2/conv3/bn/beta:0 : (1, 512)\n",
      "group1/block2/conv2/bn/gamma:0 : (1, 128)\n",
      "group1/block2/conv1/bn/gamma:0 : (1, 128)\n",
      "group1/block3/conv1/bn/gamma:0 : (1, 128)\n",
      "group1/block2/conv3/bn/gamma:0 : (1, 512)\n",
      "group1/block1/conv1/bn/beta:0 : (1, 128)\n",
      "group1/block1/conv1/bn/gamma:0 : (1, 128)\n",
      "group1/block0/convshortcut/bn/beta:0 : (1, 512)\n",
      "group1/block0/conv3/bn/beta:0 : (1, 512)\n",
      "group1/block0/conv2/bn/beta:0 : (1, 128)\n",
      "group1/block0/convshortcut/bn/gamma:0 : (1, 512)\n",
      "group1/block0/conv3/bn/gamma:0 : (1, 512)\n",
      "group2/block0/conv1/W:0 : (1, 1, 1, 512, 256)\n",
      "group1/block2/conv3/W:0 : (1, 1, 1, 128, 512)\n",
      "group1/block0/conv2/bn/gamma:0 : (1, 128)\n",
      "group1/block0/conv1/bn/beta:0 : (1, 128)\n",
      "group1/block3/conv3/W:0 : (1, 1, 1, 128, 512)\n",
      "group1/block3/conv1/W:0 : (1, 1, 1, 512, 128)\n",
      "group1/block0/conv1/bn/gamma:0 : (1, 128)\n",
      "group1/block3/conv2/W:0 : (1, 3, 3, 128, 128)\n",
      "group1/block2/conv1/W:0 : (1, 1, 1, 512, 128)\n",
      "group1/block1/conv3/W:0 : (1, 1, 1, 128, 512)\n",
      "group2/block1/conv1/W:0 : (1, 1, 1, 1024, 256)\n",
      "group1/block0/conv3/W:0 : (1, 1, 1, 128, 512)\n",
      "group1/block1/conv1/W:0 : (1, 1, 1, 512, 128)\n",
      "group2/block5/conv1/W:0 : (1, 1, 1, 1024, 256)\n",
      "rpn/conv0/W:0 : (1, 3, 3, 256, 256)\n",
      "fpn/lateral_1x1_c4/W:0 : (1, 1, 1, 1024, 256)\n",
      "group2/block3/conv1/W:0 : (1, 1, 1, 1024, 256)\n",
      "group2/block4/conv3/W:0 : (1, 1, 1, 256, 1024)\n",
      "group1/block2/conv2/W:0 : (1, 3, 3, 128, 128)\n",
      "fpn/posthoc_3x3_p4/W:0 : (1, 3, 3, 256, 256)\n",
      "group2/block1/conv3/W:0 : (1, 1, 1, 256, 1024)\n",
      "group1/block0/conv1/W:0 : (1, 1, 1, 256, 128)\n",
      "group2/block3/conv3/W:0 : (1, 1, 1, 256, 1024)\n",
      "group2/block4/conv1/W:0 : (1, 1, 1, 1024, 256)\n",
      "group2/block1/conv2/W:0 : (1, 3, 3, 256, 256)\n",
      "group2/block5/conv2/W:0 : (1, 3, 3, 256, 256)\n",
      "group2/block2/conv3/W:0 : (1, 1, 1, 256, 1024)\n",
      "group2/block0/convshortcut/W:0 : (1, 1, 1, 512, 1024)\n",
      "maskrcnn/fcn1/W:0 : (1, 3, 3, 256, 256)\n",
      "group1/block0/conv2/W:0 : (1, 3, 3, 128, 128)\n",
      "group2/block2/conv2/W:0 : (1, 3, 3, 256, 256)\n",
      "group2/block4/conv2/W:0 : (1, 3, 3, 256, 256)\n",
      "fastrcnn/fc7/W:0 : (1, 1024, 1024)\n",
      "maskrcnn/fcn0/W:0 : (1, 3, 3, 256, 256)\n",
      "fpn/posthoc_3x3_p3/W:0 : (1, 3, 3, 256, 256)\n",
      "group2/block3/conv2/W:0 : (1, 3, 3, 256, 256)\n",
      "fpn/posthoc_3x3_p2/W:0 : (1, 3, 3, 256, 256)\n",
      "maskrcnn/fcn2/W:0 : (1, 3, 3, 256, 256)\n",
      "group3/block2/conv1/W:0 : (1, 1, 1, 2048, 512)\n",
      "group2/block0/conv2/W:0 : (1, 3, 3, 256, 256)\n",
      "group3/block2/conv3/W:0 : (1, 1, 1, 512, 2048)\n",
      "group3/block1/conv1/W:0 : (1, 1, 1, 2048, 512)\n",
      "group3/block0/conv3/W:0 : (1, 1, 1, 512, 2048)\n",
      "group2/block5/conv3/W:0 : (1, 1, 1, 256, 1024)\n",
      "group2/block2/conv1/W:0 : (1, 1, 1, 1024, 256)\n",
      "group2/block0/conv3/W:0 : (1, 1, 1, 256, 1024)\n",
      "maskrcnn/fcn3/W:0 : (1, 3, 3, 256, 256)\n",
      "group1/block0/convshortcut/W:0 : (1, 1, 1, 256, 512)\n",
      "group3/block1/conv3/W:0 : (1, 1, 1, 512, 2048)\n",
      "group1/block1/conv2/W:0 : (1, 3, 3, 128, 128)\n",
      "group3/block0/conv1/W:0 : (1, 1, 1, 1024, 512)\n",
      "fpn/lateral_1x1_c5/W:0 : (1, 1, 1, 2048, 256)\n",
      "fpn/posthoc_3x3_p5/W:0 : (1, 3, 3, 256, 256)\n",
      "group3/block0/convshortcut/W:0 : (1, 1, 1, 1024, 2048)\n",
      "group3/block0/conv2/W:0 : (1, 3, 3, 512, 512)\n",
      "group3/block2/conv2/W:0 : (1, 3, 3, 512, 512)\n",
      "group3/block1/conv2/W:0 : (1, 3, 3, 512, 512)\n",
      "fastrcnn/fc6/W:0 : (1, 12544, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(\"Forward Tensor Shape\")\n",
    "for i,j in forward_tensors.items():\n",
    "    print(\"{} : {}\".format(i, np.array(j).shape))\n",
    "    \n",
    "print(\"\\nBackward Tensor Shape\")\n",
    "for i,j in backward_tensors.items():\n",
    "    print(\"{} : {}\".format(i, np.array(j).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Tensor Means\n",
      "p6 : -0.018399720606300633\n",
      "p5 : -0.020339489301798633\n",
      "p4 : -0.021849885086021547\n",
      "p3 : 0.007539847410840255\n",
      "p2 : 0.01815363196401787\n",
      "\n",
      "Backward Tensor Means\n",
      "rpn/box/b:0 : -0.008242289228333333\n",
      "rpn/box/W:0 : -0.003379818523977779\n",
      "rpn/class/b:0 : 0.14231363933333332\n",
      "rpn/conv0/b:0 : -1.7733052107812532e-05\n",
      "rpn/class/W:0 : 0.06580069587422252\n",
      "fastrcnn/outputs/box/b:0 : -2.2287111288487657e-05\n",
      "fastrcnn/outputs/class/b:0 : 2.8485185125440257e-10\n",
      "fastrcnn/fc7/b:0 : -2.4970532128134714e-05\n",
      "fastrcnn/fc6/b:0 : 9.733545261774611e-05\n",
      "maskrcnn/conv/b:0 : -0.0005258560187499999\n",
      "maskrcnn/deconv/b:0 : 0.002508225385300781\n",
      "maskrcnn/fcn3/b:0 : 0.0011247704243133204\n",
      "maskrcnn/fcn2/b:0 : 0.001305727025769375\n",
      "maskrcnn/fcn1/b:0 : 0.0010997915820076952\n",
      "maskrcnn/fcn0/b:0 : 0.0012305637350459766\n",
      "maskrcnn/conv/W:0 : -0.0006620706915804658\n",
      "fastrcnn/outputs/class/W:0 : 1.4277089529434338e-09\n",
      "fpn/posthoc_3x3_p3/b:0 : 4.9572554652070304e-05\n",
      "fpn/posthoc_3x3_p2/b:0 : 0.0002707897238558594\n",
      "fpn/posthoc_3x3_p4/b:0 : 0.00023113141660742188\n",
      "fpn/posthoc_3x3_p5/b:0 : 2.6634588611218756e-05\n",
      "fpn/lateral_1x1_c2/b:0 : -0.0002473264456445311\n",
      "fpn/lateral_1x1_c3/b:0 : -0.00036427099288828123\n",
      "fpn/lateral_1x1_c4/b:0 : -0.00044684903449218757\n",
      "fpn/lateral_1x1_c5/b:0 : -0.00048323231731328133\n",
      "group3/block2/conv3/bn/beta:0 : 5.920717798897954e-05\n",
      "group3/block2/conv2/bn/beta:0 : 0.0002801142198603516\n",
      "group3/block2/conv3/bn/gamma:0 : -4.424314828062993e-06\n",
      "group3/block2/conv2/bn/gamma:0 : 0.0003684058141097636\n",
      "group3/block2/conv1/bn/beta:0 : -0.00019864359196191403\n",
      "group3/block2/conv1/bn/gamma:0 : -0.00023330691577525393\n",
      "group3/block1/conv2/bn/beta:0 : -0.00048754751796753314\n",
      "group3/block1/conv3/bn/beta:0 : 5.5871663347899757e-05\n",
      "group3/block1/conv2/bn/gamma:0 : -0.00035491019984445305\n",
      "group3/block1/conv1/bn/beta:0 : 0.00035367656253365234\n",
      "group3/block1/conv3/bn/gamma:0 : -2.450636180100469e-05\n",
      "group3/block1/conv1/bn/gamma:0 : 0.00023453380717742184\n",
      "group3/block0/conv3/bn/beta:0 : 3.997621132611259e-05\n",
      "group3/block0/convshortcut/bn/beta:0 : 3.997621132611259e-05\n",
      "group3/block0/convshortcut/bn/gamma:0 : 9.895224660085451e-05\n",
      "group3/block0/conv2/bn/beta:0 : 0.0002801551547242383\n",
      "group3/block0/conv3/bn/gamma:0 : -3.0125944807836944e-06\n",
      "fpn/lateral_1x1_c2/W:0 : -0.0001907238282061225\n",
      "group3/block0/conv2/bn/gamma:0 : 0.0006517763968647656\n",
      "group3/block0/conv1/bn/beta:0 : 0.0004706046189849218\n",
      "fpn/lateral_1x1_c3/W:0 : -0.000175613565398565\n",
      "group3/block0/conv1/bn/gamma:0 : 0.0010298282760185253\n",
      "group2/block5/conv3/bn/beta:0 : 0.00034446023862726567\n",
      "group2/block5/conv2/bn/beta:0 : -0.0014485577132676564\n",
      "group2/block5/conv3/bn/gamma:0 : -7.19229862422412e-05\n",
      "group2/block5/conv2/bn/gamma:0 : -0.0014653330490564062\n",
      "group2/block5/conv1/bn/beta:0 : -0.0011045447811609804\n",
      "group2/block5/conv1/bn/gamma:0 : -0.0018239449878417967\n",
      "group2/block4/conv3/bn/beta:0 : 0.0003250428749346093\n",
      "group2/block4/conv2/bn/beta:0 : -0.0009663034384066016\n",
      "group2/block4/conv3/bn/gamma:0 : -7.211836995016603e-05\n",
      "group2/block4/conv2/bn/gamma:0 : -0.0007440835826347657\n",
      "group2/block4/conv1/bn/beta:0 : -0.0008066147668894142\n",
      "group2/block4/conv1/bn/gamma:0 : -0.0014208187173847656\n",
      "group2/block3/conv3/bn/beta:0 : 0.00025855982128088965\n",
      "group2/block3/conv2/bn/beta:0 : -1.0827306228906256e-06\n",
      "group2/block3/conv3/bn/gamma:0 : -2.4561475844091815e-05\n",
      "group2/block3/conv2/bn/gamma:0 : 4.899964181247267e-05\n",
      "group2/block3/conv1/bn/beta:0 : -7.551524239175783e-05\n",
      "group2/block3/conv1/bn/gamma:0 : -9.35444177991797e-05\n",
      "group2/block2/conv2/bn/beta:0 : 7.155877911015628e-05\n",
      "group2/block2/conv3/bn/beta:0 : 0.00028389267193331053\n",
      "group2/block2/conv2/bn/gamma:0 : 0.0002737212272266406\n",
      "group2/block2/conv3/bn/gamma:0 : 5.829780312897432e-05\n",
      "group2/block2/conv1/bn/beta:0 : 8.789720738906254e-05\n",
      "group2/block2/conv1/bn/gamma:0 : 0.00012253355253933587\n",
      "group2/block1/conv2/bn/beta:0 : 6.657380104570302e-05\n",
      "group2/block1/conv3/bn/beta:0 : 0.0002764164387200381\n",
      "group2/block1/conv2/bn/gamma:0 : 0.00033184591146546874\n",
      "group2/block1/conv3/bn/gamma:0 : -1.1318201530664053e-06\n",
      "group2/block1/conv1/bn/beta:0 : -0.00015800608984429686\n",
      "group2/block1/conv1/bn/gamma:0 : -0.00024086879822941406\n",
      "group2/block0/conv3/bn/beta:0 : 0.0001446762413298242\n",
      "group2/block0/convshortcut/bn/beta:0 : 0.0001446762413298242\n",
      "group2/block0/conv3/bn/gamma:0 : -0.0001170619131665918\n",
      "group2/block0/conv2/bn/beta:0 : 0.0006634422940527344\n",
      "group2/block0/convshortcut/bn/gamma:0 : 0.00016083571587873044\n",
      "group2/block0/conv2/bn/gamma:0 : 0.0006568503009699219\n",
      "group2/block0/conv1/bn/beta:0 : 0.0002978315856125\n",
      "group2/block0/conv1/bn/gamma:0 : 0.0006248014275653125\n",
      "group1/block3/conv3/bn/beta:0 : 0.00024513745308325015\n",
      "fastrcnn/outputs/box/W:0 : -5.458225724241354e-06\n",
      "group1/block3/conv3/bn/gamma:0 : 0.00012741815329451168\n",
      "group1/block3/conv2/bn/beta:0 : 0.0004338011222578123\n",
      "group1/block3/conv2/bn/gamma:0 : 0.0004586496781656249\n",
      "group1/block3/conv1/bn/beta:0 : -0.0004498352317242189\n",
      "maskrcnn/deconv/W:0 : 0.00055126817659099\n",
      "group1/block1/conv3/bn/gamma:0 : 0.0002413208049105859\n",
      "group1/block1/conv2/bn/gamma:0 : -0.0007448600228828126\n",
      "group1/block2/conv2/bn/beta:0 : 0.0008571976481078125\n",
      "group1/block2/conv1/bn/beta:0 : 0.0002701372509921875\n",
      "group1/block1/conv2/bn/beta:0 : -0.000666571563469297\n",
      "group1/block1/conv3/bn/beta:0 : 0.00023220292284\n",
      "group1/block2/conv3/bn/beta:0 : 0.0002188429115410157\n",
      "group1/block2/conv2/bn/gamma:0 : 0.0007844002493359375\n",
      "group1/block2/conv1/bn/gamma:0 : 0.0008036387634374999\n",
      "group1/block3/conv1/bn/gamma:0 : -0.0004685381891164608\n",
      "group1/block2/conv3/bn/gamma:0 : -0.00024088627124398438\n",
      "group1/block1/conv1/bn/beta:0 : -0.0010047045476726563\n",
      "group1/block1/conv1/bn/gamma:0 : -0.001314762884248203\n",
      "group1/block0/convshortcut/bn/beta:0 : 0.00020844987289582036\n",
      "group1/block0/conv3/bn/beta:0 : 0.00020844987289582036\n",
      "group1/block0/conv2/bn/beta:0 : 0.00025244067493203126\n",
      "group1/block0/convshortcut/bn/gamma:0 : 0.00015565458270513674\n",
      "group1/block0/conv3/bn/gamma:0 : 9.294413893474611e-05\n",
      "group2/block0/conv1/W:0 : 0.0002738427514041743\n",
      "group1/block2/conv3/W:0 : -4.723930671047111e-05\n",
      "group1/block0/conv2/bn/gamma:0 : -3.074468630468752e-05\n",
      "group1/block0/conv1/bn/beta:0 : 1.3947708364843718e-05\n",
      "group1/block3/conv3/W:0 : 0.00027511472918287464\n",
      "group1/block3/conv1/W:0 : -0.00027122674733339667\n",
      "group1/block0/conv1/bn/gamma:0 : -1.3863970142968861e-05\n",
      "group1/block3/conv2/W:0 : 6.72373993972954e-05\n",
      "group1/block2/conv1/W:0 : 0.00012165206155747833\n",
      "group1/block1/conv3/W:0 : 0.0001896047013949888\n",
      "group2/block1/conv1/W:0 : -0.00014330911635734286\n",
      "group1/block0/conv3/W:0 : 0.00015605961355873114\n",
      "group1/block1/conv1/W:0 : -0.0004845776767497855\n",
      "group2/block5/conv1/W:0 : -0.0003247071101174454\n",
      "rpn/conv0/W:0 : 2.0232814905784567e-06\n",
      "fpn/lateral_1x1_c4/W:0 : -9.64889859664407e-05\n",
      "group2/block3/conv1/W:0 : -3.296405388748355e-05\n",
      "group2/block4/conv3/W:0 : -5.402278404066968e-06\n",
      "group1/block2/conv2/W:0 : 0.0003939734587783908\n",
      "fpn/posthoc_3x3_p4/W:0 : -9.22188051389404e-06\n",
      "group2/block1/conv3/W:0 : 5.522082705645674e-05\n",
      "group1/block0/conv1/W:0 : 0.00029972640880722613\n",
      "group2/block3/conv3/W:0 : -2.1229015193221067e-05\n",
      "group2/block4/conv1/W:0 : -0.000326335349260094\n",
      "group2/block1/conv2/W:0 : 1.936930216201135e-06\n",
      "group2/block5/conv2/W:0 : -0.0003934486391131976\n",
      "group2/block2/conv3/W:0 : 7.630520111128909e-05\n",
      "group2/block0/convshortcut/W:0 : 0.00011370313385451983\n",
      "maskrcnn/fcn1/W:0 : 0.0010542265692204784\n",
      "group1/block0/conv2/W:0 : 5.529304615836443e-05\n",
      "group2/block2/conv2/W:0 : 3.844330187581474e-05\n",
      "group2/block4/conv2/W:0 : -0.0002202922019029083\n",
      "fastrcnn/fc7/W:0 : -2.179604363446849e-05\n",
      "maskrcnn/fcn0/W:0 : -4.430222372281532e-05\n",
      "fpn/posthoc_3x3_p3/W:0 : -9.733963192375613e-06\n",
      "group2/block3/conv2/W:0 : 1.4232418508728871e-05\n",
      "fpn/posthoc_3x3_p2/W:0 : -5.189263091835556e-05\n",
      "maskrcnn/fcn2/W:0 : 0.001180440856114996\n",
      "group3/block2/conv1/W:0 : -8.810998736054848e-05\n",
      "group2/block0/conv2/W:0 : 0.0001692125385905019\n",
      "group3/block2/conv3/W:0 : -1.8624732796083735e-05\n",
      "group3/block1/conv1/W:0 : 8.737967024460947e-05\n",
      "group3/block0/conv3/W:0 : -1.8969587824289735e-06\n",
      "group2/block5/conv3/W:0 : 1.479463250042385e-05\n",
      "group2/block2/conv1/W:0 : 1.6262731997070522e-06\n",
      "group2/block0/conv3/W:0 : -6.373531067170185e-05\n",
      "maskrcnn/fcn3/W:0 : 0.0009547520194224776\n",
      "group1/block0/convshortcut/W:0 : 0.00023181631591745426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group3/block1/conv3/W:0 : -1.0473209368506655e-05\n",
      "group1/block1/conv2/W:0 : -0.0005154865667325206\n",
      "group3/block0/conv1/W:0 : 0.00018656335364061494\n",
      "fpn/lateral_1x1_c5/W:0 : -0.0002007116821010212\n",
      "fpn/posthoc_3x3_p5/W:0 : 1.7069067191772739e-06\n",
      "group3/block0/convshortcut/W:0 : 8.965862277156866e-05\n",
      "group3/block0/conv2/W:0 : 5.533696939667889e-05\n",
      "group3/block2/conv2/W:0 : 6.098815632628358e-05\n",
      "group3/block1/conv2/W:0 : -8.88479320450196e-05\n",
      "fastrcnn/fc6/W:0 : 4.526923293837606e-07\n"
     ]
    }
   ],
   "source": [
    "print(\"Forward Tensor Means\")\n",
    "for i,j in forward_tensors.items():\n",
    "    print(\"{} : {}\".format(i, np.array(j).mean()))\n",
    "    \n",
    "print(\"\\nBackward Tensor Means\")\n",
    "for i,j in backward_tensors.items():\n",
    "    print(\"{} : {}\".format(i, np.array(j).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can save the tensors to json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/logs/forward_p23456.json', 'w') as outfile:\n",
    "    json.dump(forward_tensors, outfile)\n",
    "    \n",
    "with open('/logs/backward.json', 'w') as outfile:\n",
    "    json.dump(backward_tensors, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
