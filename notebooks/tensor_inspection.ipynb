{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook provides a set of utilities to output and inspect\n",
    "tensors as they pass through the model.\n",
    "\n",
    "\n",
    "\n",
    "This notebook details the process of identifying and tracking the values of tensors in a given network with an example using Mask RCNN.\n",
    "\n",
    "In order to run this notebook on EC2, ssh into your instance with the command\n",
    "\n",
    "ssh -i /your/ec2/keypair -L localhost:8888:localhost:8888 -L localhost:6006:localhost:6006 ec2-user@ip\n",
    "nohup jupyter notebook --no-browser --ip=0.0.0.0 > notebook.log &\n",
    "tensorboard -logdir ~/logs\n",
    "\n",
    "This notebook is broken into _ sections. First, we generate a small dataset consisting of a single image from the coco data. We then look at how to track the tensors within Mask RCNN using that image. Finally, we track the gradients that backpropogate through the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/callbacks/hooks.py:17: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/tfutils/optimizer.py:19: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/tfutils/sesscreate.py:24: The name tf.train.SessionCreator is deprecated. Please use tf.compat.v1.train.SessionCreator instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numba/errors.py:131: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod-0.18.0-py3.6-linux-x86_64.egg/horovod/tensorflow/__init__.py:117: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod-0.18.0-py3.6-linux-x86_64.egg/horovod/tensorflow/__init__.py:143: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "#os.environ['TF_CUDNN_DETERMINISTIC'] = 'true'\n",
    "os.environ['TENSORPACK_FP16'] = 'true'\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import tensorpack.utils.viz as tpviz\n",
    "from tensorpack import *\n",
    "from tensorpack.tfutils.common import get_tf_version_tuple\n",
    "sys.path.append('/mask-rcnn-tensorflow/MaskRCNN')\n",
    "from model.generalized_rcnn import ResNetFPNModel\n",
    "from config import finalize_configs, config as cfg\n",
    "from eval import DetectionResult, predict_image, multithread_predict_dataflow, EvalCallback\n",
    "from performance import ThroughputTracker, humanize_float\n",
    "from data import get_eval_dataflow, get_train_dataflow, get_batch_train_dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = ResNetFPNModel(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfg.DATA.BASEDIR = '/home/ec2-user/data'\n",
    "cfg.DATA.BASEDIR = '/data/coco/small_sample/'\n",
    "#cfg.DATA.BASEDIR = '/home/ec2-user/small_data'\n",
    "cfg.BACKBONE.WEIGHTS = '/data/coco/pretrained-models/ImageNet-R50-AlignPadding.npz'\n",
    "#cfg.MODE_FPN=True\n",
    "#cfg.FPN.NORM = 'GN'\n",
    "#cfg.TRAIN.BATCH_SIZE_PER_GPU = 4\n",
    "tf.set_random_seed(cfg.TRAIN.SEED)\n",
    "fix_rng_seed(cfg.TRAIN.SEED)\n",
    "np.random.seed(cfg.TRAIN.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train dataflow\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[1106 10:35:30 @dataset.py:50]\u001b[0m Instances loaded from /data/coco/small_sample/annotations/instances_train2017.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 4934.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1106 10:35:30 @timer.py:50]\u001b[0m Load Load annotations for train2017 finished, time:0.0068sec.\n",
      "Done loading roidbs\n",
      "\u001b[32m[1106 10:35:30 @data.py:509]\u001b[0m Filtered 0 images which contain no non-crowd groudtruth boxes. Total #images for training: 25\n",
      "Batching roidbs\n",
      "Done batching roidbs\n",
      "\u001b[32m[1106 10:35:30 @config.py:285]\u001b[0m Config: ------------------------------------------\n",
      "{'BACKBONE': {'FREEZE_AFFINE': False,\n",
      "              'FREEZE_AT': 2,\n",
      "              'NORM': 'FreezeBN',\n",
      "              'RESNET_NUM_BLOCKS': [3, 4, 6, 3],\n",
      "              'STRIDE_1X1': False,\n",
      "              'TF_PAD_MODE': False,\n",
      "              'WEIGHTS': '/data/coco/pretrained-models/ImageNet-R50-AlignPadding.npz'},\n",
      " 'DATA': {'BASEDIR': '/data/coco/small_sample/',\n",
      "          'CLASS_NAMES': ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
      "                          'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n",
      "                          'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
      "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag',\n",
      "                          'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n",
      "                          'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
      "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon',\n",
      "                          'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot',\n",
      "                          'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant',\n",
      "                          'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
      "                          'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
      "                          'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
      "                          'hair drier', 'toothbrush'],\n",
      "          'NUM_CATEGORY': 80,\n",
      "          'NUM_CLASS': 81,\n",
      "          'TRAIN': ['train2017'],\n",
      "          'VAL': ('val2017',)},\n",
      " 'FPN': {'ANCHOR_STRIDES': (4, 8, 16, 32, 64),\n",
      "         'BOXCLASS_CONV_HEAD_DIM': 256,\n",
      "         'BOXCLASS_FC_HEAD_DIM': 1024,\n",
      "         'BOXCLASS_HEAD_FUNC': 'boxclass_2fc_head',\n",
      "         'MRCNN_HEAD_FUNC': 'maskrcnn_up4conv_head',\n",
      "         'NORM': 'None',\n",
      "         'NUM_CHANNEL': 256,\n",
      "         'PROPOSAL_MODE': 'Level',\n",
      "         'RESOLUTION_REQUIREMENT': 32},\n",
      " 'FRCNN': {'BATCH_PER_IM': 512,\n",
      "           'BBOX_REG_WEIGHTS': [10.0, 10.0, 5.0, 5.0],\n",
      "           'FG_RATIO': 0.25,\n",
      "           'FG_THRESH': 0.5},\n",
      " 'MODE_FPN': True,\n",
      " 'MODE_MASK': True,\n",
      " 'MRCNN': {'HEAD_DIM': 256},\n",
      " 'PREPROC': {'MAX_SIZE': 1344.0,\n",
      "             'PADDING_SHAPES': [(800, 1000), (800, 1200), (800, 1350)],\n",
      "             'PIXEL_MEAN': [123.675, 116.28, 103.53],\n",
      "             'PIXEL_STD': [58.395, 57.12, 57.375],\n",
      "             'PREDEFINED_PADDING': False,\n",
      "             'TEST_SHORT_EDGE_SIZE': 800,\n",
      "             'TRAIN_SHORT_EDGE_SIZE': [800, 800]},\n",
      " 'RPN': {'ANCHOR_RATIOS': (0.5, 1.0, 2.0),\n",
      "         'ANCHOR_SIZES': (32, 64, 128, 256, 512),\n",
      "         'ANCHOR_STRIDE': 16,\n",
      "         'BATCH_PER_IM': 256,\n",
      "         'CROWD_OVERLAP_THRESH': 9.99,\n",
      "         'FG_RATIO': 0.5,\n",
      "         'HEAD_DIM': 1024,\n",
      "         'MIN_SIZE': 0.1,\n",
      "         'NEGATIVE_ANCHOR_THRESH': 0.3,\n",
      "         'NUM_ANCHOR': 15,\n",
      "         'POSITIVE_ANCHOR_THRESH': 0.7,\n",
      "         'PROPOSAL_NMS_THRESH': 0.7,\n",
      "         'SLOW_ACCURATE_MASK': True,\n",
      "         'TEST_PER_LEVEL_NMS_TOPK': 1000,\n",
      "         'TEST_POST_NMS_TOPK': 1000,\n",
      "         'TEST_PRE_NMS_TOPK': 6000,\n",
      "         'TOPK_PER_IMAGE': True,\n",
      "         'TRAIN_PER_LEVEL_NMS_TOPK': 2000,\n",
      "         'TRAIN_POST_NMS_TOPK': 2000,\n",
      "         'TRAIN_PRE_NMS_TOPK': 12000,\n",
      "         'UNQUANTIZED_ANCHOR': True},\n",
      " 'TEST': {'BOX_TARGET': 0.377,\n",
      "          'FRCNN_NMS_THRESH': 0.5,\n",
      "          'MASK_TARGET': 0.339,\n",
      "          'RESULTS_PER_IM': 100,\n",
      "          'RESULT_SCORE_THRESH': 0.05,\n",
      "          'RESULT_SCORE_THRESH_VIS': 0.3},\n",
      " 'TRAIN': {'BACKBONE_NCHW': True,\n",
      "           'BASE_LR': 0.00125,\n",
      "           'BATCH_SIZE_PER_GPU': 1,\n",
      "           'EVAL_PERIOD': 25,\n",
      "           'FPN_NCHW': True,\n",
      "           'GRADIENT_CLIP': 0,\n",
      "           'LR_EPOCH_SCHEDULE': [(8, 0.1), (10, 0.01), (12, None)],\n",
      "           'MASK_NCHW': True,\n",
      "           'NUM_GPUS': 1,\n",
      "           'RPN_NCHW': True,\n",
      "           'SEED': 1234,\n",
      "           'SHOULD_STOP': False,\n",
      "           'STARTING_EPOCH': 1,\n",
      "           'WARMUP_INIT_LR': 0.00041250000000000005,\n",
      "           'WARMUP_STEPS': 1000,\n",
      "           'WEIGHT_DECAY': 0.0001},\n",
      " 'TRAINER': 'replicated'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataflow = get_batch_train_dataflow(cfg.TRAIN.BATCH_SIZE_PER_GPU)\n",
    "finalize_configs(is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/tfutils/sessinit.py:259: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session_init = get_model_loader(cfg.BACKBONE.WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/tfutils/common.py:36: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "traincfg = TrainConfig(\n",
    "            model=MODEL,\n",
    "            data=QueueInput(train_dataflow),\n",
    "            steps_per_epoch=20,\n",
    "            max_epoch=1,\n",
    "            session_init=session_init,\n",
    "            session_config=None,\n",
    "            starting_epoch=cfg.TRAIN.STARTING_EPOCH\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have our data and the model is setup. In order to export a gradient from the graph, we need to add a print statement to the tensor we want to track. For example, say we want to export the second to last layer of the backbone network (c4). Add this import to the top of the backbone.py file:\n",
    "\n",
    "```from performance import print_runtime_tensor, print_runtime_tensor_loose_branch```\n",
    "\n",
    "The, just after the c4 tensor is created in the network, add this line:\n",
    "\n",
    "```c4 = print_runtime_tensor(\\\"tensor_c4_forward\\\", c4)```\n",
    "\n",
    "Similarly, say we want to output a list of tensors. Perhaps the full output of the backbone (p23456). We can use something like:\n",
    "\n",
    "```p23456 = [print_runtime_tensor(\\\"tensor_p23456_{}_forward\\\".format(i), j) for i,j in enumerate(p23456)]```\n",
    "\n",
    "On the other hand, we might want to see a tensor that isn't actually used later in the graph, which means it wouldn't normally execute such that we can output it. This can be dome using the\n",
    "\n",
    "```print_runtime_tensor_loose_branch```\n",
    "\n",
    "For this, you need a downstream trigger tesnor to force the print of the tensor of interest. Say we have a tensor `t5` that isn't used in the graph, but `t1` is. We can print `t5` with:\n",
    "\n",
    "```t1 = print_runtime_tensor_loose_branch(\\\"tensor_t5_forward\\\", t5, trigger_tensor=t1)```\n",
    "\n",
    "Finally, say we want to print the gradients of the backwards pass. This is a little more complicated. Add this gradient printer class to the generalized_rcnn.py file:\n",
    "\n",
    "```\n",
    "class GradientPrinter(tf.train.Optimizer):\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "    def compute_gradients(self, *args, **kwargs):\n",
    "        return self.opt.compute_gradients(*args, **kwargs)\n",
    "    def apply_gradients(self, gradvars, global_step=None, name=None):\n",
    "        old_grads, v = zip(*gradvars)\n",
    "        old_grads = [print_runtime_tensor(\"tensor_{}_backward\".format(i.name), j) for i,j in zip(v, old_grads)]\n",
    "        for i in v:\n",
    "            print(\"gradient_name: {}\".format(i.name))\n",
    "        gradvars = list(zip(old_grads, v))\n",
    "        return self.opt.apply_gradients(gradvars, global_step, name)\n",
    "```\n",
    "\n",
    "Inside the detection model class, modify the optimizer to pass through the gradient printer.\n",
    "\n",
    "```\n",
    "opt = tf.train.MomentumOptimizer(lr, 0.9)\n",
    "opt = GradientPrinter(opt)\n",
    "```\n",
    "\n",
    "Once the print function has been added, run the paragraph below with the capture magic function to catch the printed output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SimpleTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/MaskRCNN/model/generalized_rcnn.py:134: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/input_source/input_source.py:219: The name tf.FIFOQueue is deprecated. Please use tf.queue.FIFOQueue instead.\n",
      "\n",
      "\u001b[32m[1106 10:35:31 @input_source.py:222]\u001b[0m Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/tfutils/summary.py:266: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "\u001b[32m[1106 10:35:31 @trainers.py:49]\u001b[0m Building graph for a single training tower ...\n",
      "\u001b[32m[1106 10:35:31 @registry.py:127]\u001b[0m conv0 input: [None, 3, None, None]\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/models/conv2d.py:90: The name tf.layers.Conv2D is deprecated. Please use tf.compat.v1.layers.Conv2D instead.\n",
      "\n",
      "\u001b[32m[1106 10:35:32 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/models/batch_norm.py:196: The name tf.layers.BatchNormalization is deprecated. Please use tf.compat.v1.layers.BatchNormalization instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\u001b[32m[1106 10:35:32 @registry.py:135]\u001b[0m conv0 output: [None, 64, None, None]\n",
      "\u001b[32m[1106 10:35:32 @registry.py:127]\u001b[0m pool0 input: [None, 64, None, None]\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/models/pool.py:35: The name tf.layers.MaxPooling2D is deprecated. Please use tf.compat.v1.layers.MaxPooling2D instead.\n",
      "\n",
      "\u001b[32m[1106 10:35:32 @registry.py:135]\u001b[0m pool0 output: [None, 64, None, None]\n",
      "\u001b[32m[1106 10:35:32 @registry.py:127]\u001b[0m group0/block0/conv1 input: [None, 64, None, None]\n",
      "\u001b[32m[1106 10:35:32 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:32 @registry.py:135]\u001b[0m group0/block0/conv1 output: [None, 64, None, None]\n",
      "\u001b[32m[1106 10:35:32 @registry.py:127]\u001b[0m group0/block0/conv2 input: [None, 64, None, None]\n",
      "\u001b[32m[1106 10:35:32 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:32 @registry.py:135]\u001b[0m group0/block0/conv2 output: [None, 64, None, None]\n",
      "\u001b[32m[1106 10:35:32 @registry.py:127]\u001b[0m group0/block0/conv3 input: [None, 64, None, None]\n",
      "\u001b[32m[1106 10:35:32 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:32 @registry.py:135]\u001b[0m group0/block0/conv3 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:32 @registry.py:127]\u001b[0m group0/block0/convshortcut input: [None, 64, None, None]\n",
      "\u001b[32m[1106 10:35:32 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:32 @registry.py:135]\u001b[0m group0/block0/convshortcut output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:32 @registry.py:127]\u001b[0m group0/block1/conv1 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:32 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:32 @registry.py:135]\u001b[0m group0/block1/conv1 output: [None, 64, None, None]\n",
      "\u001b[32m[1106 10:35:32 @registry.py:127]\u001b[0m group0/block1/conv2 input: [None, 64, None, None]\n",
      "\u001b[32m[1106 10:35:32 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:32 @registry.py:135]\u001b[0m group0/block1/conv2 output: [None, 64, None, None]\n",
      "\u001b[32m[1106 10:35:32 @registry.py:127]\u001b[0m group0/block1/conv3 input: [None, 64, None, None]\n",
      "\u001b[32m[1106 10:35:32 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:32 @registry.py:135]\u001b[0m group0/block1/conv3 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:32 @registry.py:127]\u001b[0m group0/block2/conv1 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:32 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:32 @registry.py:135]\u001b[0m group0/block2/conv1 output: [None, 64, None, None]\n",
      "\u001b[32m[1106 10:35:32 @registry.py:127]\u001b[0m group0/block2/conv2 input: [None, 64, None, None]\n",
      "\u001b[32m[1106 10:35:33 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:33 @registry.py:135]\u001b[0m group0/block2/conv2 output: [None, 64, None, None]\n",
      "\u001b[32m[1106 10:35:33 @registry.py:127]\u001b[0m group0/block2/conv3 input: [None, 64, None, None]\n",
      "\u001b[32m[1106 10:35:33 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:33 @registry.py:135]\u001b[0m group0/block2/conv3 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:33 @registry.py:127]\u001b[0m group1/block0/conv1 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:33 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:33 @registry.py:135]\u001b[0m group1/block0/conv1 output: [None, 128, None, None]\n",
      "\u001b[32m[1106 10:35:33 @registry.py:127]\u001b[0m group1/block0/conv2 input: [None, 128, None, None]\n",
      "\u001b[32m[1106 10:35:33 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:33 @registry.py:135]\u001b[0m group1/block0/conv2 output: [None, 128, None, None]\n",
      "\u001b[32m[1106 10:35:33 @registry.py:127]\u001b[0m group1/block0/conv3 input: [None, 128, None, None]\n",
      "\u001b[32m[1106 10:35:33 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:33 @registry.py:135]\u001b[0m group1/block0/conv3 output: [None, 512, None, None]\n",
      "\u001b[32m[1106 10:35:33 @registry.py:127]\u001b[0m group1/block0/convshortcut input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:33 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:33 @registry.py:135]\u001b[0m group1/block0/convshortcut output: [None, 512, None, None]\n",
      "\u001b[32m[1106 10:35:33 @registry.py:127]\u001b[0m group1/block1/conv1 input: [None, 512, None, None]\n",
      "\u001b[32m[1106 10:35:33 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:33 @registry.py:135]\u001b[0m group1/block1/conv1 output: [None, 128, None, None]\n",
      "\u001b[32m[1106 10:35:33 @registry.py:127]\u001b[0m group1/block1/conv2 input: [None, 128, None, None]\n",
      "\u001b[32m[1106 10:35:33 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:33 @registry.py:135]\u001b[0m group1/block1/conv2 output: [None, 128, None, None]\n",
      "\u001b[32m[1106 10:35:33 @registry.py:127]\u001b[0m group1/block1/conv3 input: [None, 128, None, None]\n",
      "\u001b[32m[1106 10:35:33 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:33 @registry.py:135]\u001b[0m group1/block1/conv3 output: [None, 512, None, None]\n",
      "\u001b[32m[1106 10:35:33 @registry.py:127]\u001b[0m group1/block2/conv1 input: [None, 512, None, None]\n",
      "\u001b[32m[1106 10:35:33 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:33 @registry.py:135]\u001b[0m group1/block2/conv1 output: [None, 128, None, None]\n",
      "\u001b[32m[1106 10:35:33 @registry.py:127]\u001b[0m group1/block2/conv2 input: [None, 128, None, None]\n",
      "\u001b[32m[1106 10:35:33 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:33 @registry.py:135]\u001b[0m group1/block2/conv2 output: [None, 128, None, None]\n",
      "\u001b[32m[1106 10:35:33 @registry.py:127]\u001b[0m group1/block2/conv3 input: [None, 128, None, None]\n",
      "\u001b[32m[1106 10:35:33 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:33 @registry.py:135]\u001b[0m group1/block2/conv3 output: [None, 512, None, None]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1106 10:35:33 @registry.py:127]\u001b[0m group1/block3/conv1 input: [None, 512, None, None]\n",
      "\u001b[32m[1106 10:35:33 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:33 @registry.py:135]\u001b[0m group1/block3/conv1 output: [None, 128, None, None]\n",
      "\u001b[32m[1106 10:35:33 @registry.py:127]\u001b[0m group1/block3/conv2 input: [None, 128, None, None]\n",
      "\u001b[32m[1106 10:35:33 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:33 @registry.py:135]\u001b[0m group1/block3/conv2 output: [None, 128, None, None]\n",
      "\u001b[32m[1106 10:35:33 @registry.py:127]\u001b[0m group1/block3/conv3 input: [None, 128, None, None]\n",
      "\u001b[32m[1106 10:35:33 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:33 @registry.py:135]\u001b[0m group1/block3/conv3 output: [None, 512, None, None]\n",
      "\u001b[32m[1106 10:35:33 @registry.py:127]\u001b[0m group2/block0/conv1 input: [None, 512, None, None]\n",
      "\u001b[32m[1106 10:35:33 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:33 @registry.py:135]\u001b[0m group2/block0/conv1 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:33 @registry.py:127]\u001b[0m group2/block0/conv2 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:33 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:33 @registry.py:135]\u001b[0m group2/block0/conv2 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:33 @registry.py:127]\u001b[0m group2/block0/conv3 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:33 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:34 @registry.py:135]\u001b[0m group2/block0/conv3 output: [None, 1024, None, None]\n",
      "\u001b[32m[1106 10:35:34 @registry.py:127]\u001b[0m group2/block0/convshortcut input: [None, 512, None, None]\n",
      "\u001b[32m[1106 10:35:34 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:34 @registry.py:135]\u001b[0m group2/block0/convshortcut output: [None, 1024, None, None]\n",
      "\u001b[32m[1106 10:35:34 @registry.py:127]\u001b[0m group2/block1/conv1 input: [None, 1024, None, None]\n",
      "\u001b[32m[1106 10:35:34 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:34 @registry.py:135]\u001b[0m group2/block1/conv1 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:34 @registry.py:127]\u001b[0m group2/block1/conv2 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:34 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:34 @registry.py:135]\u001b[0m group2/block1/conv2 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:34 @registry.py:127]\u001b[0m group2/block1/conv3 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:34 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:34 @registry.py:135]\u001b[0m group2/block1/conv3 output: [None, 1024, None, None]\n",
      "\u001b[32m[1106 10:35:34 @registry.py:127]\u001b[0m group2/block2/conv1 input: [None, 1024, None, None]\n",
      "\u001b[32m[1106 10:35:34 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:34 @registry.py:135]\u001b[0m group2/block2/conv1 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:34 @registry.py:127]\u001b[0m group2/block2/conv2 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:34 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:34 @registry.py:135]\u001b[0m group2/block2/conv2 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:34 @registry.py:127]\u001b[0m group2/block2/conv3 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:34 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:34 @registry.py:135]\u001b[0m group2/block2/conv3 output: [None, 1024, None, None]\n",
      "\u001b[32m[1106 10:35:34 @registry.py:127]\u001b[0m group2/block3/conv1 input: [None, 1024, None, None]\n",
      "\u001b[32m[1106 10:35:34 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:34 @registry.py:135]\u001b[0m group2/block3/conv1 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:34 @registry.py:127]\u001b[0m group2/block3/conv2 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:34 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:34 @registry.py:135]\u001b[0m group2/block3/conv2 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:34 @registry.py:127]\u001b[0m group2/block3/conv3 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:34 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:34 @registry.py:135]\u001b[0m group2/block3/conv3 output: [None, 1024, None, None]\n",
      "\u001b[32m[1106 10:35:34 @registry.py:127]\u001b[0m group2/block4/conv1 input: [None, 1024, None, None]\n",
      "\u001b[32m[1106 10:35:34 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:34 @registry.py:135]\u001b[0m group2/block4/conv1 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:34 @registry.py:127]\u001b[0m group2/block4/conv2 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:34 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:34 @registry.py:135]\u001b[0m group2/block4/conv2 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:34 @registry.py:127]\u001b[0m group2/block4/conv3 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:34 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:34 @registry.py:135]\u001b[0m group2/block4/conv3 output: [None, 1024, None, None]\n",
      "\u001b[32m[1106 10:35:34 @registry.py:127]\u001b[0m group2/block5/conv1 input: [None, 1024, None, None]\n",
      "\u001b[32m[1106 10:35:34 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:34 @registry.py:135]\u001b[0m group2/block5/conv1 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:34 @registry.py:127]\u001b[0m group2/block5/conv2 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:34 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:34 @registry.py:135]\u001b[0m group2/block5/conv2 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:34 @registry.py:127]\u001b[0m group2/block5/conv3 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:34 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:34 @registry.py:135]\u001b[0m group2/block5/conv3 output: [None, 1024, None, None]\n",
      "\u001b[32m[1106 10:35:34 @registry.py:127]\u001b[0m group3/block0/conv1 input: [None, 1024, None, None]\n",
      "\u001b[32m[1106 10:35:35 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m group3/block0/conv1 output: [None, 512, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m group3/block0/conv2 input: [None, 512, None, None]\n",
      "\u001b[32m[1106 10:35:35 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m group3/block0/conv2 output: [None, 512, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m group3/block0/conv3 input: [None, 512, None, None]\n",
      "\u001b[32m[1106 10:35:35 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m group3/block0/conv3 output: [None, 2048, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m group3/block0/convshortcut input: [None, 1024, None, None]\n",
      "\u001b[32m[1106 10:35:35 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m group3/block0/convshortcut output: [None, 2048, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m group3/block1/conv1 input: [None, 2048, None, None]\n",
      "\u001b[32m[1106 10:35:35 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m group3/block1/conv1 output: [None, 512, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m group3/block1/conv2 input: [None, 512, None, None]\n",
      "\u001b[32m[1106 10:35:35 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m group3/block1/conv2 output: [None, 512, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m group3/block1/conv3 input: [None, 512, None, None]\n",
      "\u001b[32m[1106 10:35:35 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m group3/block1/conv3 output: [None, 2048, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m group3/block2/conv1 input: [None, 2048, None, None]\n",
      "\u001b[32m[1106 10:35:35 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m group3/block2/conv1 output: [None, 512, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m group3/block2/conv2 input: [None, 512, None, None]\n",
      "\u001b[32m[1106 10:35:35 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m group3/block2/conv2 output: [None, 512, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m group3/block2/conv3 input: [None, 512, None, None]\n",
      "\u001b[32m[1106 10:35:35 @batch_norm.py:174]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m [BatchNorm] Using moving_mean/moving_variance in training.\n",
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m group3/block2/conv3 output: [None, 2048, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m fpn input: [None, 256, None, None],[None, 512, None, None],[None, 1024, None, None],[None, 2048, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m fpn/lateral_1x1_c2 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m fpn/lateral_1x1_c2 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m fpn/lateral_1x1_c3 input: [None, 512, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m fpn/lateral_1x1_c3 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m fpn/lateral_1x1_c4 input: [None, 1024, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m fpn/lateral_1x1_c4 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m fpn/lateral_1x1_c5 input: [None, 2048, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m fpn/lateral_1x1_c5 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m fpn/upsample_lat5 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m fpn/upsample_lat5 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m fpn/upsample_lat4 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m fpn/upsample_lat4 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m fpn/upsample_lat3 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m fpn/upsample_lat3 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m fpn/posthoc_3x3_p2 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m fpn/posthoc_3x3_p2 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m fpn/posthoc_3x3_p3 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m fpn/posthoc_3x3_p3 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m fpn/posthoc_3x3_p4 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m fpn/posthoc_3x3_p4 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m fpn/posthoc_3x3_p5 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m fpn/posthoc_3x3_p5 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m fpn/maxpool_p6 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m fpn/maxpool_p6 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:135]\u001b[0m fpn output: [None, 256, None, None],[None, 256, None, None],[None, 256, None, None],[None, 256, None, None],[None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m rpn input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:35 @registry.py:127]\u001b[0m rpn/conv0 input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:36 @registry.py:135]\u001b[0m rpn/conv0 output: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:36 @registry.py:127]\u001b[0m rpn/class input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:36 @registry.py:135]\u001b[0m rpn/class output: [None, 3, None, None]\n",
      "\u001b[32m[1106 10:35:36 @registry.py:127]\u001b[0m rpn/box input: [None, 256, None, None]\n",
      "\u001b[32m[1106 10:35:36 @registry.py:135]\u001b[0m rpn/box output: [None, 12, None, None]\n",
      "\u001b[32m[1106 10:35:36 @registry.py:135]\u001b[0m rpn output: [None, None, None, 3],[None, 12, None, None]\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/MaskRCNN/model/rpn.py:123: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
      "\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/MaskRCNN/model/rpn.py:125: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
      "\n",
      "\u001b[32m[1106 10:35:37 @registry.py:127]\u001b[0m fastrcnn input: [None, 256, 7, 7]\n",
      "\u001b[32m[1106 10:35:37 @registry.py:127]\u001b[0m fastrcnn/fc6 input: [None, 256, 7, 7]\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/models/fc.py:59: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n",
      "\n",
      "\u001b[32m[1106 10:35:38 @registry.py:135]\u001b[0m fastrcnn/fc6 output: [None, 1024]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:127]\u001b[0m fastrcnn/fc7 input: [None, 1024]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:135]\u001b[0m fastrcnn/fc7 output: [None, 1024]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:135]\u001b[0m fastrcnn output: [None, 1024]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:127]\u001b[0m fastrcnn/outputs input: [None, 1024]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:127]\u001b[0m fastrcnn/outputs/class input: [None, 1024]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:135]\u001b[0m fastrcnn/outputs/class output: [None, 81]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:127]\u001b[0m fastrcnn/outputs/box input: [None, 1024]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:135]\u001b[0m fastrcnn/outputs/box output: [None, 324]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:135]\u001b[0m fastrcnn/outputs output: [None, 81],[None, 81, 4]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:127]\u001b[0m maskrcnn input: [None, 256, 14, 14]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:127]\u001b[0m maskrcnn/fcn0 input: [None, 256, 14, 14]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:135]\u001b[0m maskrcnn/fcn0 output: [None, 256, 14, 14]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:127]\u001b[0m maskrcnn/fcn1 input: [None, 256, 14, 14]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:135]\u001b[0m maskrcnn/fcn1 output: [None, 256, 14, 14]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:127]\u001b[0m maskrcnn/fcn2 input: [None, 256, 14, 14]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:135]\u001b[0m maskrcnn/fcn2 output: [None, 256, 14, 14]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:127]\u001b[0m maskrcnn/fcn3 input: [None, 256, 14, 14]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:135]\u001b[0m maskrcnn/fcn3 output: [None, 256, 14, 14]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:127]\u001b[0m maskrcnn/deconv input: [None, 256, 14, 14]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:135]\u001b[0m maskrcnn/deconv output: [None, 256, 28, 28]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:127]\u001b[0m maskrcnn/conv input: [None, 256, 28, 28]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:135]\u001b[0m maskrcnn/conv output: [None, 80, 28, 28]\n",
      "\u001b[32m[1106 10:35:38 @registry.py:135]\u001b[0m maskrcnn output: [None, 80, 28, 28]\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/MaskRCNN/model_box.py:195: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/MaskRCNN/model/mask_head.py:36: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name mask_truth|pred is illegal; using mask_truth_pred instead.\n",
      "\u001b[32m[1106 10:35:39 @regularize.py:97]\u001b[0m regularize_cost() found 63 variables to regularize.\n",
      "\u001b[32m[1106 10:35:39 @regularize.py:22]\u001b[0m The following tensors will be regularized: group1/block0/conv1/W:0, group1/block0/conv2/W:0, group1/block0/conv3/W:0, group1/block0/convshortcut/W:0, group1/block1/conv1/W:0, group1/block1/conv2/W:0, group1/block1/conv3/W:0, group1/block2/conv1/W:0, group1/block2/conv2/W:0, group1/block2/conv3/W:0, group1/block3/conv1/W:0, group1/block3/conv2/W:0, group1/block3/conv3/W:0, group2/block0/conv1/W:0, group2/block0/conv2/W:0, group2/block0/conv3/W:0, group2/block0/convshortcut/W:0, group2/block1/conv1/W:0, group2/block1/conv2/W:0, group2/block1/conv3/W:0, group2/block2/conv1/W:0, group2/block2/conv2/W:0, group2/block2/conv3/W:0, group2/block3/conv1/W:0, group2/block3/conv2/W:0, group2/block3/conv3/W:0, group2/block4/conv1/W:0, group2/block4/conv2/W:0, group2/block4/conv3/W:0, group2/block5/conv1/W:0, group2/block5/conv2/W:0, group2/block5/conv3/W:0, group3/block0/conv1/W:0, group3/block0/conv2/W:0, group3/block0/conv3/W:0, group3/block0/convshortcut/W:0, group3/block1/conv1/W:0, group3/block1/conv2/W:0, group3/block1/conv3/W:0, group3/block2/conv1/W:0, group3/block2/conv2/W:0, group3/block2/conv3/W:0, fpn/lateral_1x1_c2/W:0, fpn/lateral_1x1_c3/W:0, fpn/lateral_1x1_c4/W:0, fpn/lateral_1x1_c5/W:0, fpn/posthoc_3x3_p2/W:0, fpn/posthoc_3x3_p3/W:0, fpn/posthoc_3x3_p4/W:0, fpn/posthoc_3x3_p5/W:0, rpn/conv0/W:0, rpn/class/W:0, rpn/box/W:0, fastrcnn/fc6/W:0, fastrcnn/fc7/W:0, fastrcnn/outputs/class/W:0, fastrcnn/outputs/box/W:0, maskrcnn/fcn0/W:0, maskrcnn/fcn1/W:0, maskrcnn/fcn2/W:0, maskrcnn/fcn3/W:0, maskrcnn/deconv/W:0, maskrcnn/conv/W:0\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/MaskRCNN/model/generalized_rcnn.py:80: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1106 10:35:44 @monitor.py:259]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m logger directory was not set. Ignore TFEventWriter.\n",
      "\u001b[32m[1106 10:35:44 @monitor.py:300]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m logger directory was not set. Ignore JSONWriter.\n",
      "\u001b[32m[1106 10:35:44 @model_utils.py:66]\u001b[0m \u001b[36mTrainable Variables: \n",
      "\u001b[0mname                                   shape                    dim\n",
      "-------------------------------------  ------------------  --------\n",
      "group1/block0/conv1/W:0                [1, 1, 256, 128]       32768\n",
      "group1/block0/conv1/bn/gamma:0         [128]                    128\n",
      "group1/block0/conv1/bn/beta:0          [128]                    128\n",
      "group1/block0/conv2/W:0                [3, 3, 128, 128]      147456\n",
      "group1/block0/conv2/bn/gamma:0         [128]                    128\n",
      "group1/block0/conv2/bn/beta:0          [128]                    128\n",
      "group1/block0/conv3/W:0                [1, 1, 128, 512]       65536\n",
      "group1/block0/conv3/bn/gamma:0         [512]                    512\n",
      "group1/block0/conv3/bn/beta:0          [512]                    512\n",
      "group1/block0/convshortcut/W:0         [1, 1, 256, 512]      131072\n",
      "group1/block0/convshortcut/bn/gamma:0  [512]                    512\n",
      "group1/block0/convshortcut/bn/beta:0   [512]                    512\n",
      "group1/block1/conv1/W:0                [1, 1, 512, 128]       65536\n",
      "group1/block1/conv1/bn/gamma:0         [128]                    128\n",
      "group1/block1/conv1/bn/beta:0          [128]                    128\n",
      "group1/block1/conv2/W:0                [3, 3, 128, 128]      147456\n",
      "group1/block1/conv2/bn/gamma:0         [128]                    128\n",
      "group1/block1/conv2/bn/beta:0          [128]                    128\n",
      "group1/block1/conv3/W:0                [1, 1, 128, 512]       65536\n",
      "group1/block1/conv3/bn/gamma:0         [512]                    512\n",
      "group1/block1/conv3/bn/beta:0          [512]                    512\n",
      "group1/block2/conv1/W:0                [1, 1, 512, 128]       65536\n",
      "group1/block2/conv1/bn/gamma:0         [128]                    128\n",
      "group1/block2/conv1/bn/beta:0          [128]                    128\n",
      "group1/block2/conv2/W:0                [3, 3, 128, 128]      147456\n",
      "group1/block2/conv2/bn/gamma:0         [128]                    128\n",
      "group1/block2/conv2/bn/beta:0          [128]                    128\n",
      "group1/block2/conv3/W:0                [1, 1, 128, 512]       65536\n",
      "group1/block2/conv3/bn/gamma:0         [512]                    512\n",
      "group1/block2/conv3/bn/beta:0          [512]                    512\n",
      "group1/block3/conv1/W:0                [1, 1, 512, 128]       65536\n",
      "group1/block3/conv1/bn/gamma:0         [128]                    128\n",
      "group1/block3/conv1/bn/beta:0          [128]                    128\n",
      "group1/block3/conv2/W:0                [3, 3, 128, 128]      147456\n",
      "group1/block3/conv2/bn/gamma:0         [128]                    128\n",
      "group1/block3/conv2/bn/beta:0          [128]                    128\n",
      "group1/block3/conv3/W:0                [1, 1, 128, 512]       65536\n",
      "group1/block3/conv3/bn/gamma:0         [512]                    512\n",
      "group1/block3/conv3/bn/beta:0          [512]                    512\n",
      "group2/block0/conv1/W:0                [1, 1, 512, 256]      131072\n",
      "group2/block0/conv1/bn/gamma:0         [256]                    256\n",
      "group2/block0/conv1/bn/beta:0          [256]                    256\n",
      "group2/block0/conv2/W:0                [3, 3, 256, 256]      589824\n",
      "group2/block0/conv2/bn/gamma:0         [256]                    256\n",
      "group2/block0/conv2/bn/beta:0          [256]                    256\n",
      "group2/block0/conv3/W:0                [1, 1, 256, 1024]     262144\n",
      "group2/block0/conv3/bn/gamma:0         [1024]                  1024\n",
      "group2/block0/conv3/bn/beta:0          [1024]                  1024\n",
      "group2/block0/convshortcut/W:0         [1, 1, 512, 1024]     524288\n",
      "group2/block0/convshortcut/bn/gamma:0  [1024]                  1024\n",
      "group2/block0/convshortcut/bn/beta:0   [1024]                  1024\n",
      "group2/block1/conv1/W:0                [1, 1, 1024, 256]     262144\n",
      "group2/block1/conv1/bn/gamma:0         [256]                    256\n",
      "group2/block1/conv1/bn/beta:0          [256]                    256\n",
      "group2/block1/conv2/W:0                [3, 3, 256, 256]      589824\n",
      "group2/block1/conv2/bn/gamma:0         [256]                    256\n",
      "group2/block1/conv2/bn/beta:0          [256]                    256\n",
      "group2/block1/conv3/W:0                [1, 1, 256, 1024]     262144\n",
      "group2/block1/conv3/bn/gamma:0         [1024]                  1024\n",
      "group2/block1/conv3/bn/beta:0          [1024]                  1024\n",
      "group2/block2/conv1/W:0                [1, 1, 1024, 256]     262144\n",
      "group2/block2/conv1/bn/gamma:0         [256]                    256\n",
      "group2/block2/conv1/bn/beta:0          [256]                    256\n",
      "group2/block2/conv2/W:0                [3, 3, 256, 256]      589824\n",
      "group2/block2/conv2/bn/gamma:0         [256]                    256\n",
      "group2/block2/conv2/bn/beta:0          [256]                    256\n",
      "group2/block2/conv3/W:0                [1, 1, 256, 1024]     262144\n",
      "group2/block2/conv3/bn/gamma:0         [1024]                  1024\n",
      "group2/block2/conv3/bn/beta:0          [1024]                  1024\n",
      "group2/block3/conv1/W:0                [1, 1, 1024, 256]     262144\n",
      "group2/block3/conv1/bn/gamma:0         [256]                    256\n",
      "group2/block3/conv1/bn/beta:0          [256]                    256\n",
      "group2/block3/conv2/W:0                [3, 3, 256, 256]      589824\n",
      "group2/block3/conv2/bn/gamma:0         [256]                    256\n",
      "group2/block3/conv2/bn/beta:0          [256]                    256\n",
      "group2/block3/conv3/W:0                [1, 1, 256, 1024]     262144\n",
      "group2/block3/conv3/bn/gamma:0         [1024]                  1024\n",
      "group2/block3/conv3/bn/beta:0          [1024]                  1024\n",
      "group2/block4/conv1/W:0                [1, 1, 1024, 256]     262144\n",
      "group2/block4/conv1/bn/gamma:0         [256]                    256\n",
      "group2/block4/conv1/bn/beta:0          [256]                    256\n",
      "group2/block4/conv2/W:0                [3, 3, 256, 256]      589824\n",
      "group2/block4/conv2/bn/gamma:0         [256]                    256\n",
      "group2/block4/conv2/bn/beta:0          [256]                    256\n",
      "group2/block4/conv3/W:0                [1, 1, 256, 1024]     262144\n",
      "group2/block4/conv3/bn/gamma:0         [1024]                  1024\n",
      "group2/block4/conv3/bn/beta:0          [1024]                  1024\n",
      "group2/block5/conv1/W:0                [1, 1, 1024, 256]     262144\n",
      "group2/block5/conv1/bn/gamma:0         [256]                    256\n",
      "group2/block5/conv1/bn/beta:0          [256]                    256\n",
      "group2/block5/conv2/W:0                [3, 3, 256, 256]      589824\n",
      "group2/block5/conv2/bn/gamma:0         [256]                    256\n",
      "group2/block5/conv2/bn/beta:0          [256]                    256\n",
      "group2/block5/conv3/W:0                [1, 1, 256, 1024]     262144\n",
      "group2/block5/conv3/bn/gamma:0         [1024]                  1024\n",
      "group2/block5/conv3/bn/beta:0          [1024]                  1024\n",
      "group3/block0/conv1/W:0                [1, 1, 1024, 512]     524288\n",
      "group3/block0/conv1/bn/gamma:0         [512]                    512\n",
      "group3/block0/conv1/bn/beta:0          [512]                    512\n",
      "group3/block0/conv2/W:0                [3, 3, 512, 512]     2359296\n",
      "group3/block0/conv2/bn/gamma:0         [512]                    512\n",
      "group3/block0/conv2/bn/beta:0          [512]                    512\n",
      "group3/block0/conv3/W:0                [1, 1, 512, 2048]    1048576\n",
      "group3/block0/conv3/bn/gamma:0         [2048]                  2048\n",
      "group3/block0/conv3/bn/beta:0          [2048]                  2048\n",
      "group3/block0/convshortcut/W:0         [1, 1, 1024, 2048]   2097152\n",
      "group3/block0/convshortcut/bn/gamma:0  [2048]                  2048\n",
      "group3/block0/convshortcut/bn/beta:0   [2048]                  2048\n",
      "group3/block1/conv1/W:0                [1, 1, 2048, 512]    1048576\n",
      "group3/block1/conv1/bn/gamma:0         [512]                    512\n",
      "group3/block1/conv1/bn/beta:0          [512]                    512\n",
      "group3/block1/conv2/W:0                [3, 3, 512, 512]     2359296\n",
      "group3/block1/conv2/bn/gamma:0         [512]                    512\n",
      "group3/block1/conv2/bn/beta:0          [512]                    512\n",
      "group3/block1/conv3/W:0                [1, 1, 512, 2048]    1048576\n",
      "group3/block1/conv3/bn/gamma:0         [2048]                  2048\n",
      "group3/block1/conv3/bn/beta:0          [2048]                  2048\n",
      "group3/block2/conv1/W:0                [1, 1, 2048, 512]    1048576\n",
      "group3/block2/conv1/bn/gamma:0         [512]                    512\n",
      "group3/block2/conv1/bn/beta:0          [512]                    512\n",
      "group3/block2/conv2/W:0                [3, 3, 512, 512]     2359296\n",
      "group3/block2/conv2/bn/gamma:0         [512]                    512\n",
      "group3/block2/conv2/bn/beta:0          [512]                    512\n",
      "group3/block2/conv3/W:0                [1, 1, 512, 2048]    1048576\n",
      "group3/block2/conv3/bn/gamma:0         [2048]                  2048\n",
      "group3/block2/conv3/bn/beta:0          [2048]                  2048\n",
      "fpn/lateral_1x1_c2/W:0                 [1, 1, 256, 256]       65536\n",
      "fpn/lateral_1x1_c2/b:0                 [256]                    256\n",
      "fpn/lateral_1x1_c3/W:0                 [1, 1, 512, 256]      131072\n",
      "fpn/lateral_1x1_c3/b:0                 [256]                    256\n",
      "fpn/lateral_1x1_c4/W:0                 [1, 1, 1024, 256]     262144\n",
      "fpn/lateral_1x1_c4/b:0                 [256]                    256\n",
      "fpn/lateral_1x1_c5/W:0                 [1, 1, 2048, 256]     524288\n",
      "fpn/lateral_1x1_c5/b:0                 [256]                    256\n",
      "fpn/posthoc_3x3_p2/W:0                 [3, 3, 256, 256]      589824\n",
      "fpn/posthoc_3x3_p2/b:0                 [256]                    256\n",
      "fpn/posthoc_3x3_p3/W:0                 [3, 3, 256, 256]      589824\n",
      "fpn/posthoc_3x3_p3/b:0                 [256]                    256\n",
      "fpn/posthoc_3x3_p4/W:0                 [3, 3, 256, 256]      589824\n",
      "fpn/posthoc_3x3_p4/b:0                 [256]                    256\n",
      "fpn/posthoc_3x3_p5/W:0                 [3, 3, 256, 256]      589824\n",
      "fpn/posthoc_3x3_p5/b:0                 [256]                    256\n",
      "rpn/conv0/W:0                          [3, 3, 256, 256]      589824\n",
      "rpn/conv0/b:0                          [256]                    256\n",
      "rpn/class/W:0                          [1, 1, 256, 3]           768\n",
      "rpn/class/b:0                          [3]                        3\n",
      "rpn/box/W:0                            [1, 1, 256, 12]         3072\n",
      "rpn/box/b:0                            [12]                      12\n",
      "fastrcnn/fc6/W:0                       [12544, 1024]       12845056\n",
      "fastrcnn/fc6/b:0                       [1024]                  1024\n",
      "fastrcnn/fc7/W:0                       [1024, 1024]         1048576\n",
      "fastrcnn/fc7/b:0                       [1024]                  1024\n",
      "fastrcnn/outputs/class/W:0             [1024, 81]             82944\n",
      "fastrcnn/outputs/class/b:0             [81]                      81\n",
      "fastrcnn/outputs/box/W:0               [1024, 324]           331776\n",
      "fastrcnn/outputs/box/b:0               [324]                    324\n",
      "maskrcnn/fcn0/W:0                      [3, 3, 256, 256]      589824\n",
      "maskrcnn/fcn0/b:0                      [256]                    256\n",
      "maskrcnn/fcn1/W:0                      [3, 3, 256, 256]      589824\n",
      "maskrcnn/fcn1/b:0                      [256]                    256\n",
      "maskrcnn/fcn2/W:0                      [3, 3, 256, 256]      589824\n",
      "maskrcnn/fcn2/b:0                      [256]                    256\n",
      "maskrcnn/fcn3/W:0                      [3, 3, 256, 256]      589824\n",
      "maskrcnn/fcn3/b:0                      [256]                    256\n",
      "maskrcnn/deconv/W:0                    [2, 2, 256, 256]      262144\n",
      "maskrcnn/deconv/b:0                    [256]                    256\n",
      "maskrcnn/conv/W:0                      [1, 1, 256, 80]        20480\n",
      "maskrcnn/conv/b:0                      [80]                      80\u001b[36m\n",
      "Total #vars=168, #params=44175092, size=168.51MB\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1106 10:35:44 @base.py:210]\u001b[0m Setup callbacks graph ...\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/callbacks/graph.py:56: The name tf.train.SessionRunArgs is deprecated. Please use tf.estimator.SessionRunArgs instead.\n",
      "\n",
      "\u001b[32m[1106 10:35:44 @argtools.py:148]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m \"import prctl\" failed! Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[1106 10:35:44 @summary.py:48]\u001b[0m [MovingAverageSummary] 27 operations in collection 'MOVING_SUMMARY_OPS' will be run with session hooks.\n",
      "\u001b[32m[1106 10:35:44 @summary.py:95]\u001b[0m Summarizing collection 'summaries' of size 30.\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/callbacks/summary.py:96: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "\u001b[32m[1106 10:35:44 @base.py:231]\u001b[0m Creating the session ...\n",
      "\u001b[32m[1106 10:35:53 @base.py:237]\u001b[0m Initializing the session ...\n",
      "\u001b[32m[1106 10:35:53 @sessinit.py:206]\u001b[0m Variables to restore from dict: group2/block5/conv1/W:0, group1/block2/conv1/bn/variance/EMA:0, group3/block2/conv3/bn/mean/EMA:0, group2/block3/conv2/bn/gamma:0, group3/block1/conv3/bn/mean/EMA:0, group1/block2/conv3/bn/beta:0, group3/block2/conv2/bn/beta:0, group0/block1/conv3/bn/mean/EMA:0, group0/block1/conv1/W:0, group1/block3/conv1/bn/variance/EMA:0, group2/block1/conv1/bn/beta:0, group2/block0/convshortcut/W:0, group2/block0/conv3/bn/beta:0, group3/block1/conv2/bn/mean/EMA:0, group3/block1/conv2/bn/beta:0, group3/block0/conv3/W:0, group2/block3/conv1/bn/mean/EMA:0, group3/block0/conv1/bn/mean/EMA:0, group1/block2/conv2/bn/variance/EMA:0, group1/block3/conv2/bn/gamma:0, group2/block5/conv3/W:0, group2/block2/conv1/bn/gamma:0, group1/block3/conv3/bn/variance/EMA:0, group3/block0/conv2/bn/gamma:0, group1/block3/conv1/bn/beta:0, group3/block1/conv2/bn/variance/EMA:0, group0/block2/conv3/W:0, group2/block4/conv1/bn/beta:0, group2/block0/convshortcut/bn/gamma:0, group3/block0/conv2/W:0, group0/block1/conv1/bn/gamma:0, group2/block1/conv2/bn/mean/EMA:0, group2/block4/conv3/W:0, group0/block1/conv2/bn/variance/EMA:0, group2/block5/conv2/bn/mean/EMA:0, group3/block1/conv1/bn/gamma:0, group2/block0/conv2/bn/mean/EMA:0, group1/block0/conv3/bn/gamma:0, group0/block0/conv2/bn/beta:0, group1/block1/conv3/bn/beta:0, group2/block0/conv2/bn/beta:0, group3/block0/conv1/bn/beta:0, group1/block2/conv3/bn/variance/EMA:0, group3/block0/convshortcut/bn/variance/EMA:0, group1/block1/conv1/bn/beta:0, group3/block2/conv3/W:0, group1/block1/conv2/bn/beta:0, group0/block0/conv3/bn/mean/EMA:0, group2/block5/conv1/bn/mean/EMA:0, group3/block2/conv2/bn/mean/EMA:0, group2/block4/conv3/bn/beta:0, group1/block0/conv1/bn/beta:0, conv0/bn/mean/EMA:0, group1/block2/conv2/bn/beta:0, group3/block2/conv1/W:0, group1/block3/conv2/bn/variance/EMA:0, group2/block5/conv2/bn/beta:0, group2/block3/conv3/W:0, group2/block1/conv1/bn/mean/EMA:0, group3/block0/conv3/bn/mean/EMA:0, group2/block3/conv1/W:0, group1/block0/conv1/bn/gamma:0, group2/block5/conv1/bn/beta:0, group2/block3/conv3/bn/beta:0, group1/block1/conv3/bn/gamma:0, group2/block5/conv2/bn/variance/EMA:0, group0/block2/conv3/bn/variance/EMA:0, conv0/W:0, group3/block0/conv1/bn/gamma:0, group2/block1/conv3/bn/variance/EMA:0, group0/block2/conv1/W:0, group3/block2/conv1/bn/beta:0, group2/block3/conv3/bn/variance/EMA:0, group1/block0/conv1/bn/mean/EMA:0, group2/block1/conv3/bn/mean/EMA:0, group0/block1/conv3/bn/gamma:0, group3/block1/conv3/bn/beta:0, group0/block0/conv1/bn/gamma:0, group2/block4/conv1/bn/variance/EMA:0, group2/block4/conv3/bn/gamma:0, group3/block1/conv2/bn/gamma:0, group1/block3/conv1/bn/gamma:0, group0/block0/conv2/W:0, group3/block1/conv3/bn/gamma:0, group1/block0/conv1/bn/variance/EMA:0, group1/block0/conv1/W:0, group1/block1/conv2/bn/gamma:0, group3/block1/conv1/bn/mean/EMA:0, group1/block0/conv2/bn/gamma:0, group0/block2/conv1/bn/variance/EMA:0, group1/block0/conv3/bn/variance/EMA:0, group3/block0/conv1/W:0, group1/block1/conv1/bn/gamma:0, conv0/bn/variance/EMA:0, group3/block0/conv3/bn/beta:0, group2/block2/conv3/W:0, group1/block2/conv3/bn/gamma:0, group1/block2/conv1/W:0, group0/block0/convshortcut/bn/gamma:0, group3/block2/conv3/bn/variance/EMA:0, group1/block3/conv2/bn/mean/EMA:0, group2/block4/conv2/bn/mean/EMA:0, group2/block0/conv3/bn/gamma:0, group1/block0/convshortcut/bn/mean/EMA:0, group0/block0/conv3/W:0, group2/block2/conv2/W:0, group0/block1/conv3/bn/variance/EMA:0, group1/block2/conv2/bn/gamma:0, group2/block5/conv3/bn/gamma:0, group2/block1/conv2/bn/gamma:0, group3/block2/conv3/bn/gamma:0, group0/block1/conv3/W:0, group2/block5/conv1/bn/variance/EMA:0, group2/block2/conv3/bn/mean/EMA:0, group1/block0/conv3/W:0, group2/block5/conv3/bn/beta:0, group2/block4/conv2/bn/gamma:0, group0/block2/conv2/bn/gamma:0, group2/block4/conv3/bn/variance/EMA:0, group1/block1/conv3/W:0, group0/block1/conv2/bn/beta:0, group2/block3/conv2/bn/mean/EMA:0, group2/block1/conv1/bn/variance/EMA:0, group2/block2/conv2/bn/variance/EMA:0, group1/block0/conv2/bn/variance/EMA:0, group2/block0/conv1/W:0, group3/block2/conv1/bn/gamma:0, group2/block2/conv2/bn/beta:0, group2/block4/conv2/bn/beta:0, group2/block2/conv2/bn/mean/EMA:0, group2/block1/conv1/bn/gamma:0, group0/block0/conv3/bn/beta:0, group0/block0/conv2/bn/variance/EMA:0, group1/block3/conv2/bn/beta:0, group0/block2/conv2/bn/mean/EMA:0, group1/block3/conv1/bn/mean/EMA:0, group1/block1/conv2/W:0, group3/block2/conv2/bn/gamma:0, group1/block0/convshortcut/bn/variance/EMA:0, group2/block2/conv1/bn/variance/EMA:0, group2/block2/conv3/bn/beta:0, group2/block0/conv1/bn/variance/EMA:0, group2/block0/convshortcut/bn/beta:0, group1/block1/conv2/bn/variance/EMA:0, group2/block1/conv2/W:0, group0/block0/convshortcut/bn/beta:0, group3/block0/conv2/bn/beta:0, group2/block2/conv3/bn/variance/EMA:0, group1/block0/conv3/bn/mean/EMA:0, group0/block0/conv1/bn/mean/EMA:0, group1/block3/conv1/W:0, group2/block4/conv1/W:0, group3/block0/conv2/bn/variance/EMA:0, group2/block3/conv1/bn/beta:0, group2/block2/conv1/bn/beta:0, group2/block3/conv1/bn/gamma:0, group2/block1/conv2/bn/variance/EMA:0, group0/block0/conv1/bn/variance/EMA:0, group0/block2/conv1/bn/mean/EMA:0, group2/block2/conv2/bn/gamma:0, group2/block2/conv1/W:0, group2/block5/conv2/bn/gamma:0, group1/block3/conv3/bn/mean/EMA:0, group1/block1/conv3/bn/variance/EMA:0, group0/block1/conv2/W:0, group2/block0/conv2/bn/gamma:0, group3/block0/conv2/bn/mean/EMA:0, group2/block0/convshortcut/bn/variance/EMA:0, group3/block2/conv2/W:0, group0/block0/convshortcut/bn/mean/EMA:0, group0/block1/conv2/bn/gamma:0, group0/block2/conv1/bn/gamma:0, group1/block3/conv3/bn/gamma:0, group1/block2/conv2/bn/mean/EMA:0, group0/block2/conv3/bn/beta:0, group2/block0/conv3/W:0, group3/block2/conv1/bn/mean/EMA:0, group1/block1/conv3/bn/mean/EMA:0, group3/block2/conv3/bn/beta:0, group2/block1/conv3/bn/beta:0, group2/block2/conv3/bn/gamma:0, group0/block0/conv2/bn/gamma:0, group2/block4/conv1/bn/mean/EMA:0, group3/block2/conv1/bn/variance/EMA:0, group2/block0/convshortcut/bn/mean/EMA:0, group2/block5/conv2/W:0, group2/block3/conv2/bn/beta:0, group2/block3/conv3/bn/mean/EMA:0, group0/block2/conv3/bn/mean/EMA:0, group1/block2/conv1/bn/gamma:0, group2/block5/conv3/bn/variance/EMA:0, group3/block0/convshortcut/bn/beta:0, group2/block5/conv3/bn/mean/EMA:0, group2/block0/conv2/bn/variance/EMA:0, group2/block0/conv1/bn/beta:0, group1/block3/conv2/W:0, conv0/bn/beta:0, group1/block1/conv1/bn/variance/EMA:0, group1/block0/convshortcut/W:0, group3/block1/conv1/bn/variance/EMA:0, group2/block0/conv3/bn/mean/EMA:0, group1/block2/conv3/bn/mean/EMA:0, group1/block0/convshortcut/bn/gamma:0, group3/block0/convshortcut/W:0, group1/block1/conv1/bn/mean/EMA:0, group1/block0/conv2/bn/mean/EMA:0, group1/block2/conv1/bn/mean/EMA:0, group2/block0/conv1/bn/mean/EMA:0, group1/block1/conv1/W:0, group0/block0/conv3/bn/variance/EMA:0, group3/block0/conv3/bn/gamma:0, group2/block1/conv3/bn/gamma:0, group0/block0/convshortcut/bn/variance/EMA:0, group3/block0/convshortcut/bn/mean/EMA:0, group0/block2/conv2/bn/beta:0, group0/block0/conv3/bn/gamma:0, group0/block1/conv1/bn/variance/EMA:0, group3/block0/convshortcut/bn/gamma:0, conv0/bn/gamma:0, group2/block4/conv2/bn/variance/EMA:0, group2/block3/conv2/bn/variance/EMA:0, group0/block2/conv3/bn/gamma:0, group0/block2/conv2/W:0, group1/block3/conv3/W:0, group2/block2/conv1/bn/mean/EMA:0, group0/block0/conv2/bn/mean/EMA:0, group2/block4/conv2/W:0, group0/block2/conv2/bn/variance/EMA:0, group1/block3/conv3/bn/beta:0, group1/block0/conv3/bn/beta:0, group3/block0/conv3/bn/variance/EMA:0, group0/block1/conv3/bn/beta:0, group1/block2/conv2/W:0, group2/block1/conv2/bn/beta:0, group2/block3/conv1/bn/variance/EMA:0, group3/block1/conv2/W:0, group0/block1/conv2/bn/mean/EMA:0, group3/block2/conv2/bn/variance/EMA:0, group1/block1/conv2/bn/mean/EMA:0, group3/block1/conv3/W:0, group1/block0/convshortcut/bn/beta:0, group2/block5/conv1/bn/gamma:0, group1/block2/conv3/W:0, group0/block0/conv1/W:0, group2/block3/conv3/bn/gamma:0, group2/block1/conv3/W:0, group1/block2/conv1/bn/beta:0, group0/block1/conv1/bn/mean/EMA:0, group0/block0/convshortcut/W:0, group2/block0/conv1/bn/gamma:0, group2/block4/conv1/bn/gamma:0, group2/block4/conv3/bn/mean/EMA:0, group0/block1/conv1/bn/beta:0, group0/block2/conv1/bn/beta:0, group2/block1/conv1/W:0, group2/block3/conv2/W:0, group1/block0/conv2/bn/beta:0, group3/block1/conv1/bn/beta:0, group1/block0/conv2/W:0, group3/block1/conv3/bn/variance/EMA:0, group3/block0/conv1/bn/variance/EMA:0, group2/block0/conv2/W:0, group3/block1/conv1/W:0, group0/block0/conv1/bn/beta:0, group2/block0/conv3/bn/variance/EMA:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1106 10:35:53 @sessinit.py:89]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m The following variables are in the graph, but not found in the dict: fastrcnn/fc6/W, fastrcnn/fc6/b, fastrcnn/fc7/W, fastrcnn/fc7/b, fastrcnn/outputs/box/W, fastrcnn/outputs/box/b, fastrcnn/outputs/class/W, fastrcnn/outputs/class/b, fpn/lateral_1x1_c2/W, fpn/lateral_1x1_c2/b, fpn/lateral_1x1_c3/W, fpn/lateral_1x1_c3/b, fpn/lateral_1x1_c4/W, fpn/lateral_1x1_c4/b, fpn/lateral_1x1_c5/W, fpn/lateral_1x1_c5/b, fpn/posthoc_3x3_p2/W, fpn/posthoc_3x3_p2/b, fpn/posthoc_3x3_p3/W, fpn/posthoc_3x3_p3/b, fpn/posthoc_3x3_p4/W, fpn/posthoc_3x3_p4/b, fpn/posthoc_3x3_p5/W, fpn/posthoc_3x3_p5/b, global_step, learning_rate, maskrcnn/conv/W, maskrcnn/conv/b, maskrcnn/deconv/W, maskrcnn/deconv/b, maskrcnn/fcn0/W, maskrcnn/fcn0/b, maskrcnn/fcn1/W, maskrcnn/fcn1/b, maskrcnn/fcn2/W, maskrcnn/fcn2/b, maskrcnn/fcn3/W, maskrcnn/fcn3/b, rpn/box/W, rpn/box/b, rpn/class/W, rpn/class/b, rpn/conv0/W, rpn/conv0/b\n",
      "\u001b[32m[1106 10:35:53 @sessinit.py:89]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m The following variables are in the dict, but not found in the graph: linear/W, linear/b\n",
      "\u001b[32m[1106 10:35:53 @sessinit.py:219]\u001b[0m Restoring 265 variables from dict ...\n",
      "WARNING:tensorflow:From /mask-rcnn-tensorflow/tensorpack/tfutils/varmanip.py:108: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[32m[1106 10:35:56 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block0/convshortcut/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1106 10:35:57 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block1/conv2/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1106 10:35:59 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block2/conv3/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1106 10:36:00 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block0/conv2/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1106 10:36:00 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable conv0/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1106 10:36:01 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block0/conv1/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1106 10:36:02 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block2/conv1/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1106 10:36:04 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block0/conv3/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1106 10:36:08 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block1/conv3/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1106 10:36:08 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block2/conv2/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1106 10:36:14 @varmanip.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Variable group0/block1/conv1/W has dtype <dtype: 'float16'> but was given a value of dtype float32. Load it after downcasting!\n",
      "\u001b[32m[1106 10:36:19 @base.py:244]\u001b[0m Graph Finalized.\n",
      "\u001b[32m[1106 10:36:19 @concurrency.py:40]\u001b[0m Starting EnqueueThread QueueInput/input_queue ...\n",
      "\u001b[32m[1106 10:36:19 @base.py:276]\u001b[0m Start Epoch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########|20/20[00:26<00:00, 0.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1106 10:36:46 @base.py:286]\u001b[0m Epoch 1 (global_step 20) finished, time:26.7 seconds.\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m QueueInput/queue_size: 50\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m boxclass_losses/box_loss: 0.11954\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m boxclass_losses/label_loss: 0.842\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m boxclass_losses/label_metrics/accuracy: 0.91671\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m boxclass_losses/label_metrics/false_negative: 0.97059\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m boxclass_losses/label_metrics/fg_accuracy: 0\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m boxclass_losses/num_fg_label: 27.88\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m learning_rate: 0.003\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m maskrcnn_loss/accuracy: 0.52806\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m maskrcnn_loss/fg_pixel_ratio: 0.55169\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m maskrcnn_loss/maskrcnn_loss: 0.82215\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m maskrcnn_loss/pos_accuracy: 0.38167\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level2: 170.01\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level3: 190.85\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level4: 133.28\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m multilevel_roi_align/fpn_map_rois_to_levels/num_roi_level5: 17.864\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level2: 6.953\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level3: 8.4868\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level4: 8.5583\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m multilevel_roi_align_mask/fpn_map_rois_to_levels/num_roi_level5: 3.882\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m rpn_losses/box_loss: 0.072705\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m rpn_losses/label_loss: 0.28557\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m sample_fast_rcnn_targets/num_bg: 484.12\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m sample_fast_rcnn_targets/num_fg: 27.88\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m sample_fast_rcnn_targets/proposal_metrics/best_iou_per_gt: 0.55188\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m sample_fast_rcnn_targets/proposal_metrics/recall_iou0.3: 0.87764\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m sample_fast_rcnn_targets/proposal_metrics/recall_iou0.5: 0.62593\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m total_cost: 2.7666\n",
      "\u001b[32m[1106 10:36:46 @monitor.py:469]\u001b[0m wd_cost: 0.62463\n",
      "\u001b[32m[1106 10:36:46 @base.py:290]\u001b[0m Training has finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%capture cap_out --no-stderr\n",
    "launch_train_with_config(traincfg, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Use channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nc2345 (<tf.Tensor 'group0/block2/output:0' shape=(?, 256, ?, ?) dtype=float16>, <tf.Tensor 'group1/block3/output:0' shape=(?, 512, ?, ?) dtype=float16>, <tf.Tensor 'group2/block5/output:0' shape=(?, 1024, ?, ?) dtype=float16>, <tf.Tensor 'group3/block2/output:0' shape=(?, 2048, ?, ?) dtype=float16>)\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nUse channels_first data format\\nTENSORPACK_FP16 set. Using FP16 loss scaling of 1024.0\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1106 10:36:46 @input_source.py:178]\u001b[0m EnqueueThread QueueInput/input_queue Exited.\n"
     ]
    }
   ],
   "source": [
    "cap_out.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
